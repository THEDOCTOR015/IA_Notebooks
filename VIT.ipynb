{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIT\n",
    "\n",
    "MS COCO 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import layers, models, losses, callbacks\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from pycocotools.coco import COCO\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(gpus)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes et variables globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "ANNOTDIR = 'annotations_trainval2014'\n",
    "DATADIR = 'train2014'\n",
    "INSTANCEFILE = '{}/annotations/instances_{}.json'.format(ANNOTDIR, DATADIR)\n",
    "\n",
    "# Hyper-paramètres\n",
    "RATIO_TRAIN = 0.8\n",
    "RATIO_VAL = 0.15\n",
    "RATIO_TEST = 0.05\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "PATIENCE = 3\n",
    "EMBEDDING_DIM = 256\n",
    "COCO_INSTANCES = COCO(INSTANCEFILE)\n",
    "NUM_TOTAL_CLASSES = 90 # 80 classes + 10 classes omises\n",
    "\n",
    "# Vérifications\n",
    "assert RATIO_TRAIN + RATIO_VAL + RATIO_TEST == 1 # Vérification de la somme des ratios\n",
    "\n",
    "num_classes = len(COCO_INSTANCES.getCatIds())\n",
    "print(f'Nombre de classes dans le dataset COCO: {num_classes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator(Sequence):\n",
    "    def _getsplit(self, ensemble):\n",
    "        if ensemble == 'train':\n",
    "            start = 0\n",
    "            stop = int(RATIO_TRAIN * len(self.imgIds))\n",
    "        elif ensemble == 'val':\n",
    "            start = int(RATIO_TRAIN * len(self.imgIds))\n",
    "            stop = int((RATIO_TRAIN + RATIO_VAL) * len(self.imgIds))\n",
    "        elif ensemble == 'test':\n",
    "            start = int((RATIO_TRAIN + RATIO_VAL) * len(self.imgIds))\n",
    "            stop = len(self.imgIds)\n",
    "        return start, stop\n",
    "\n",
    "    def __init__(self, ensemble, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.ensemble = ensemble\n",
    "        \n",
    "        # Créer une liste de tous les IDs d'images\n",
    "        self.imgIds = COCO_INSTANCES.getImgIds()\n",
    "        start, stop = self._getsplit(ensemble)\n",
    "        self.ids = self.imgIds[start:stop]\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.ids) / BATCH_SIZE))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_ids = self.ids[index * BATCH_SIZE : (index + 1) * BATCH_SIZE]\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "        for id in batch_ids:\n",
    "            # Charger l'image\n",
    "            file_name = COCO_INSTANCES.imgs[id]['file_name']\n",
    "            image = Image.open(f'{DATADIR}/{file_name}')\n",
    "            image = image.resize((224, 224))\n",
    "            image = image.convert('RGB')\n",
    "            image = np.array(image)\n",
    "            batch_images.append(image)\n",
    "            # Charger les classes\n",
    "            annIds = COCO_INSTANCES.getAnnIds(imgIds=id)\n",
    "            anns = COCO_INSTANCES.loadAnns(annIds)\n",
    "            labels = [0.0 for _ in range(NUM_TOTAL_CLASSES)]\n",
    "            for ann in anns:\n",
    "                labels[ann['category_id']] = 1.0\n",
    "            batch_labels.append(labels)\n",
    "\n",
    "        batch_labels = np.array(batch_labels)\n",
    "        batch_images = np.array(batch_images) / 255.0\n",
    "\n",
    "        return (batch_images, batch_labels)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.ids = np.random.permutation(self.ids)\n",
    "\n",
    "train_generator = DatasetGenerator('train')\n",
    "val_generator = DatasetGenerator('val')\n",
    "test_generator = DatasetGenerator('test')\n",
    "\n",
    "print(f'Taille du dataset d\\'entrainement: {len(train_generator)} batches, {len(train_generator.ids)} items')\n",
    "print(f'Taille du dataset de validation: {len(val_generator)} batches, {len(val_generator.ids)} items')\n",
    "print(f'Taille du dataset de test: {len(test_generator)} batches, {len(test_generator.ids)} items')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test unitaire du générateur de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = train_generator\n",
    "# Récupérer un batch d'images et de légendes\n",
    "r_index = np.random.randint(len(generator))\n",
    "images, labels = generator.__getitem__(r_index-1)\n",
    "# Extraire une image et ses classes\n",
    "r_index = np.random.randint(len(images))\n",
    "image = images[r_index]\n",
    "label = labels[r_index]\n",
    "label_ids= [str(i) for i in np.where(label == 1)[0]]\n",
    "label_str = ', '.join([ COCO_INSTANCES.cats[int(i)]['name'] for i in label_ids])\n",
    "# Afficher une image et ses classes\n",
    "plt.imshow(image)\n",
    "plt.title(f'Classes: {label_str}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### CUSTOM LAYERS ###\n",
    "\n",
    "\n",
    "\n",
    "### CUSTOM MODELS ###\n",
    "\n",
    "def VIT_v1():\n",
    "    '''\n",
    "    inputs :\n",
    "    - image : (224, 224, 3)\n",
    "    outputs :\n",
    "    - class : (80)\n",
    "    '''\n",
    "    # Création du modèle\n",
    "    image_input = layers.Input(shape=(224, 224, 3))\n",
    "\n",
    "    model = Model(inputs=image_input, outputs=output, name='VIT_v1')\n",
    "model = VIT_v1()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file=f'{model.name}.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                         patience=PATIENCE, \n",
    "                                         restore_best_weights=True)\n",
    "\n",
    "checkpoint_path = f'checkpoints/{model.name}'\n",
    "checkpoint_path = checkpoint_path + '-{epoch:04d}.keras'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "model_checkpoint = callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    validation_data=val_generator,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=1)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sauvegarde du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'Livrable3_{model.name}.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
