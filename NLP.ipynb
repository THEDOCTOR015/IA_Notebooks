{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NPL\n",
    "Utilise Word2Vec, LSTM et ce dataset :  SMS Spam Collection Dataset = https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import random as r\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import recall_score, accuracy_score, fbeta_score, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('spam.csv', encoding=\"ISO-8859-1\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Télécharger la liste des stop words\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Fonction de nettoyage de texte\n",
    "def clean_text(text):\n",
    "    # Retirer les caractères non-alphabétiques et convertir en minuscules\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Retirer les stop words\n",
    "    stopped_tokens = [w for w in tokens if w not in stopwords.words('english')]\n",
    "    return ' '.join(stopped_tokens)\n",
    "\n",
    "# Appliquer la fonction de nettoyage à chaque message\n",
    "data['v2'] = data['v2'].apply(clean_text)\n",
    "data.head()\n",
    "print(\"Word count: \", data['v2'].apply(lambda x: len(x.split(' '))).sum())\n",
    "print(\"Word Token: \", data['v2'].apply(lambda x: len(word_tokenize(x))).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement de Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Préparer les données pour Word2Vec (liste de listes de mots)\n",
    "sentences = [message.split() for message in data['v2']]\n",
    "\n",
    "# Entraîner un modèle Word2Vec\n",
    "wordvec = Word2Vec(sentences, vector_size=50, window=4, min_count=1, workers=3, epochs=100)\n",
    "\n",
    "# Fonction pour vectoriser un message en utilisant les vecteurs de ses mots\n",
    "def vectorize_message(message):\n",
    "    words = message.split()  # Tokenisation simple\n",
    "    # On filtre les mots qui n'ont pas de vecteur associé\n",
    "    word_vectors = [wordvec.wv[word] for word in words if word in wordvec.wv]  # Obtention des vecteurs \n",
    "    return word_vectors\n",
    "\n",
    "# Appliquer la vectorisation à chaque message\n",
    "data['Vector'] = data['v2'].apply(vectorize_message)\n",
    "data.head()\n",
    "\n",
    "# Convertir la liste de vecteurs en array numpy pour le padding\n",
    "sequences = pad_sequences(data['Vector'].tolist(), padding='post', dtype='float32', value=0.0)\n",
    "\n",
    "print(\"Shape of sequences: \", sequences.shape)\n",
    "\n",
    "# Préparer les labels\n",
    "labels = data['v1'].apply(lambda x: 1 if x == 'spam' else 0).values\n",
    "\n",
    "print(\"Spam count: \", np.sum(labels))\n",
    "print(\"Ham count: \", len(labels) - np.sum(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    input = layers.Input(shape=(sequences.shape[1], sequences.shape[2]))\n",
    "    x = layers.Masking(mask_value=0.0)(input) # Extrèmement important\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LSTM(64)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(16, activation='leaky_relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(1, activation='sigmoid')(x)\n",
    "    return models.Model(inputs=input, outputs=x, name='spam_detector')\n",
    "\n",
    "model = model()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Callback d'early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Entraîner le modèle\n",
    "history = model.fit(sequences, labels, batch_size=64, epochs=30, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse de l'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tracer l'historique de l'entraînement\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Tracer la perte\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Perte d\\'entraînement')\n",
    "plt.plot(history.history['val_loss'], label='Perte de validation')\n",
    "plt.title('Perte')\n",
    "plt.xlabel('Épochs')\n",
    "plt.ylabel('Perte')\n",
    "plt.legend()\n",
    "\n",
    "# Tracer l'exactitude\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Exactitude d\\'entraînement')\n",
    "plt.plot(history.history['val_accuracy'], label='Exactitude de validation')\n",
    "plt.title('Exactitude')\n",
    "plt.xlabel('Épochs')\n",
    "plt.ylabel('Exactitude')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédire les labels pour les données d'entraînement\n",
    "predictions = model.predict(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation du modèle avec matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Threshold = 0.5\n",
    "Beta = 1.0\n",
    "\n",
    "predicted_labels = (predictions > Threshold).astype(int).flatten()\n",
    "\n",
    "# Calculer le recall\n",
    "recall = recall_score(labels, predicted_labels)\n",
    "print(f\"Recall: {recall}\")\n",
    "# Calculer l'accuracy\n",
    "accuracy = accuracy_score(labels, predicted_labels)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "# Calculer le F-beta score\n",
    "f_beta = fbeta_score(labels, predicted_labels, beta=Beta)\n",
    "print(f\"F-{Beta} Score: {f_beta}\")\n",
    "\n",
    "# Calculer la matrice de confusion\n",
    "cm = confusion_matrix(labels, predicted_labels)\n",
    "\n",
    "# Afficher la matrice de confusion\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['ham', 'spam'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation du modèle avec une courbe ROC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculer les valeurs de la courbe ROC\n",
    "fpr, tpr, thresholds = roc_curve(labels, predictions)\n",
    "\n",
    "# Calculer l'AUC (Area Under Curve)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Tracer la courbe ROC\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='Courbe ROC (aire = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title('Courbe ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test du modèle avec input utilisateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "def predict_spam(message):\n",
    "    # Nettoyer le message\n",
    "    cleaned_message = clean_text(message)\n",
    "    \n",
    "    # Vectoriser le message\n",
    "    vectorized_message = vectorize_message(cleaned_message)\n",
    "    print(\"Length of vectorized message: \", len(vectorized_message))\n",
    "    \n",
    "    # Appliquer le padding\n",
    "    padded_message = pad_sequences([vectorized_message], maxlen=sequences.shape[1], padding='post', truncating='post', dtype='float32', value=0.0)\n",
    "    \n",
    "    # Prédire avec le modèle\n",
    "    prediction = model.predict(padded_message)\n",
    "    \n",
    "    # Retourner le résultat\n",
    "    return 'spam' if prediction > Threshold else 'ham'\n",
    "\n",
    "\n",
    "\n",
    "    # Exemple d'utilisation\n",
    "\n",
    "    message1 = \"Congratulations! You've won a free ticket to the Bahamas. Text WIN to 12345 to claim your prize.\"\n",
    "message2 = \"Hey, what are you doing later? Want to grab a cup of coffee?\"\n",
    "message3 = \"Buy cheap viagra cialis online\"\n",
    "# 332 caractères\n",
    "message4 = \"Hey motherfucker, ahhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\"\n",
    "result = predict_spam(message4)\n",
    "print(f\"The message is classified as: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
