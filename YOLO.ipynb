{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, Constants and Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IMPORTS\n",
    "'''\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import os\n",
    "import keras\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import Sequence , plot_model\n",
    "from tensorflow.keras.models import load_model\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import absl.logging\n",
    "import logging\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from PIL import Image\n",
    "from tqdm import tqdm # progress bar\n",
    "\n",
    "\n",
    "\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "logging.getLogger('tensorflow').setLevel(logging.WARNING)\n",
    "#tf.config.run_functions_eagerly(True)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(gpus, \"Physical GPUs,\", logical_gpus, \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "'''\n",
    "CONSTANTS\n",
    "'''\n",
    "\n",
    "# CONSTANTES\n",
    "DATASET_FOLDER = 'datasets/yolo' # Dataset folder with annotations and images\n",
    "DATASET_SPLIT = (0.80,0.15,0.05) # Split between train, val and test\n",
    "LABELS_FOLDER = DATASET_FOLDER + '/labels'\n",
    "IMAGES_FOLDER = DATASET_FOLDER + '/images'\n",
    "IMAGE_SIZE = (576, 1024) # Heigth, Width\n",
    "MODEL_EPOCHS = 1000 # Number of epoch\n",
    "MODEL_CELLULES = (18,32) # Number of cells heigth, width (line,column)\n",
    "MODEL_BATCH_SIZE = 2 # batch size\n",
    "MODEL_LEARNING_RATE = 1e-4\n",
    "MODEL_PATIENCE = 5 \n",
    "\n",
    "MODEL_ANCHOR_BOXES = [(7.771702728797546, 11.099425402414909), (16.233133106872444, 4.676295486695418), (26.65295286429116, 2.322343759539281), (9.476821452651933, 3.7753047321480744), (3.642066698646129, 5.355965495956248), (5.794567115957937, 2.5241804259133427), (3.5527940911190563, 2.2905150829398315), (1.7457785313564547, 2.229531520410109), (1.2712882556016014, 1.0891510755676512)] # Heigth, Width with k=9 in k-means\n",
    "\n",
    "MODEL_LAMBDA_CLASS = 1\n",
    "MODEL_LAMBDA_OBJ = 1\n",
    "\n",
    "MODEL_LAMBDA_COORD = 4\n",
    "MODEL_LAMBDA_IOU = 0\n",
    "MODEL_LAMBDA_NOOBJ = 1 \n",
    "\n",
    "THRESHOLD_CONFIDENCE = 0.7 # Seuil de confiance pour la détection d'objet\n",
    "MODEL_MINIMAL_IOU = 0.5 # Seuil minimal d'IOU pour considérer une détection comme correcte\n",
    "MODEL_GRID_SENSIBILITY_COEF = 1.2 # Coefficient d'extension de la sigmoid pour x,y\n",
    "MODEL_SIGMOID_MULTIPLIER = 1 # Multiplicateur de la sigmoid pour w,h\n",
    "MODEL_SIGMOID_ADDER = 0.5 # Ajout à la sigmoid pour w,h\n",
    "\n",
    "MODEL_CELLULES_SIZE = (IMAGE_SIZE[0]/MODEL_CELLULES[0], IMAGE_SIZE[1]/MODEL_CELLULES[1]) # Taille d'une cellule en pixels\n",
    "MODEL_ANCHOR_BOXES_PIXELSIZE = [(MODEL_CELLULES_SIZE[0]*anchor[1], MODEL_CELLULES_SIZE[1]*anchor[0]) for anchor in MODEL_ANCHOR_BOXES] # Taille des anchor boxes en pixels (hauteur,largeur)\n",
    "MODEL_ANCHOR_BOXES_PIXELSIZE_2 = [(anchor[0]/2,anchor[1]/2) for anchor in MODEL_ANCHOR_BOXES_PIXELSIZE] # Taille des anchor boxes en pixels (divisée par 2)\n",
    "MODEL_CELLULES_SIZE_INV = (1/MODEL_CELLULES_SIZE[0], 1/MODEL_CELLULES_SIZE[1]) # % d'une cellule en pixels (utile pour les calculs de coordonnées)\n",
    "MODEL_ANCHOR_BOXES_COLOR = [(np.random.rand(),np.random.rand(),np.random.rand()) for i in range(len(MODEL_ANCHOR_BOXES))] # Couleurs des anchor boxes\n",
    "\n",
    "dataset_filepath = np.array([ (image, label) for image, label in zip(os.listdir(IMAGES_FOLDER), os.listdir(LABELS_FOLDER)) ])\n",
    "dataset_indices = np.arange(len(dataset_filepath))\n",
    "np.random.shuffle(dataset_indices)\n",
    "\n",
    "'''\n",
    "UTILS\n",
    "'''\n",
    "def select_generator():\n",
    "    selected_input = input('Choose the generator (train, val, test) : ')\n",
    "    if selected_input == 'train' :\n",
    "        return train_generator\n",
    "    elif selected_input == 'val' :\n",
    "        return val_generator\n",
    "    elif selected_input == 'test' :\n",
    "        return test_generator\n",
    "    else :\n",
    "        raise ValueError('Unknown subset (train, val, test)')\n",
    "                           \n",
    "# Fonction de dessin des prédictions\n",
    "def draw_predict(image, y_pred=None, y_true=None, showprederrors=True, nms=False, only_true=False):\n",
    "    def sigmoid(x):\n",
    "        x=np.clip(x, -50, 50)\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    if only_true and y_true is None:\n",
    "        print(\"Erreur : only_true est True mais y_true est None\")\n",
    "        return\n",
    "    fig, ax = plt.subplots(1)\n",
    "    # Séparer les prédictions\n",
    "    if y_pred is not None:\n",
    "        pred_boxes=y_pred[..., 0:4]\n",
    "        pred_conf=tf.sigmoid(y_pred[..., 4])\n",
    "        # Afficher l'image avec les bounding boxes\n",
    "        nb_anchor = 0\n",
    "        #distribution_anchor = [0 for i in range(len(MODEL_ANCHOR_BOXES))]\n",
    "        distribution_anchor_conf = [0 for i in range(len(MODEL_ANCHOR_BOXES))]\n",
    "        # Dessiner les lignes verticales et horizontales pour les cellules\n",
    "        for i in range(MODEL_CELLULES[1] + 1):  # Lignes verticales\n",
    "            ax.axvline(x=i * MODEL_CELLULES_SIZE[1], color='w', linestyle='-', linewidth=0.1)\n",
    "        for j in range(MODEL_CELLULES[0] + 1):  # Lignes horizontales\n",
    "            ax.axhline(y=j * MODEL_CELLULES_SIZE[0], color='w', linestyle='-', linewidth=0.1)\n",
    "        # Ajouter les prédictions des boîtes englobantes à l'image\n",
    "        for i in range(MODEL_CELLULES[0]):\n",
    "            for j in range(MODEL_CELLULES[1]):\n",
    "                show_box = True\n",
    "                if only_true :\n",
    "                    show_box = False if np.sum(y_true[i,j,:,4]) == 0 else True\n",
    "                if nms and show_box :\n",
    "                    k_max = np.argmax(pred_conf[i, j])\n",
    "                    box = pred_boxes[i, j, k_max]\n",
    "                    if pred_conf[i,j,k_max] > THRESHOLD_CONFIDENCE :\n",
    "                        # Conversion de la box prédite en coordonnées\n",
    "                        x_center = (j + MODEL_GRID_SENSIBILITY_COEF*sigmoid(box[0]) - (MODEL_GRID_SENSIBILITY_COEF-1)/2) * MODEL_CELLULES_SIZE[1]\n",
    "                        y_center = (i + MODEL_GRID_SENSIBILITY_COEF*sigmoid(box[1]) - (MODEL_GRID_SENSIBILITY_COEF-1)/2) * MODEL_CELLULES_SIZE[0]\n",
    "                        w_box = (MODEL_SIGMOID_MULTIPLIER*sigmoid(box[2]) + MODEL_SIGMOID_ADDER ) * MODEL_ANCHOR_BOXES[k_max][0] * MODEL_CELLULES_SIZE[1]\n",
    "                        h_box = (MODEL_SIGMOID_MULTIPLIER*sigmoid(box[3]) + MODEL_SIGMOID_ADDER ) * MODEL_ANCHOR_BOXES[k_max][1] * MODEL_CELLULES_SIZE[0]\n",
    "                        # Calcul des coins de la box\n",
    "                        x_min = x_center - w_box / 2\n",
    "                        y_min = y_center - h_box / 2\n",
    "                        # Ajout des données à la distribution\n",
    "                        nb_anchor += 1\n",
    "                        distribution_anchor_conf[k_max] += 1\n",
    "                        # Dessiner la boîte englobante\n",
    "                        rect=patches.Rectangle((x_min, y_min), w_box, h_box, linewidth=1, edgecolor=MODEL_ANCHOR_BOXES_COLOR[k_max], facecolor='none')\n",
    "                        ax.add_patch(rect)\n",
    "                        ax.scatter(x_center, y_center, color=MODEL_ANCHOR_BOXES_COLOR[k_max], s=2)\n",
    "                elif show_box :\n",
    "                    for k in range(len(MODEL_ANCHOR_BOXES)):\n",
    "                        box = pred_boxes[i, j, k]\n",
    "                        if pred_conf[i, j, k] > THRESHOLD_CONFIDENCE :\n",
    "                            # Conversion de la box prédite en coordonnées\n",
    "                            x_center = (j + MODEL_GRID_SENSIBILITY_COEF*sigmoid(box[0]) - (MODEL_GRID_SENSIBILITY_COEF-1)/2) * MODEL_CELLULES_SIZE[1]\n",
    "                            y_center = (i + MODEL_GRID_SENSIBILITY_COEF*sigmoid(box[1]) - (MODEL_GRID_SENSIBILITY_COEF-1)/2) * MODEL_CELLULES_SIZE[0]\n",
    "                            #w_box = np.exp(box[2]) * MODEL_ANCHOR_BOXES[k][0] * MODEL_CELLULES_SIZE[1] # A AMELIORER\n",
    "                            w_box = (MODEL_SIGMOID_MULTIPLIER*sigmoid(box[2]) + MODEL_SIGMOID_ADDER )* MODEL_ANCHOR_BOXES[k][0] * MODEL_CELLULES_SIZE[1]\n",
    "                            #h_box = np.exp(box[3]) * MODEL_ANCHOR_BOXES[k][1] * MODEL_CELLULES_SIZE[0] # A AMELIORER\n",
    "                            h_box = (MODEL_SIGMOID_MULTIPLIER*sigmoid(box[3]) + MODEL_SIGMOID_ADDER )* MODEL_ANCHOR_BOXES[k][1] * MODEL_CELLULES_SIZE[0]\n",
    "                                \n",
    "                            # Calcul des coins de la box\n",
    "                            x_min = x_center - w_box / 2\n",
    "                            y_min = y_center - h_box / 2\n",
    "                            # Ajout des données à la distribution\n",
    "                            nb_anchor += 1\n",
    "                            distribution_anchor_conf[k] += 1\n",
    "                            # Dessiner la boîte englobante\n",
    "                            rect=patches.Rectangle((x_min, y_min), w_box, h_box, linewidth=1, edgecolor=MODEL_ANCHOR_BOXES_COLOR[k], facecolor='none')\n",
    "                            ax.add_patch(rect)\n",
    "                            ax.scatter(x_center, y_center, color=MODEL_ANCHOR_BOXES_COLOR[k], s=2)\n",
    "\n",
    "    # Ajouter les vérités terrain à l'image\n",
    "    if y_true is not None and not only_true:\n",
    "        # On converti les coordonnées des vérités terrain\n",
    "        for i in range(MODEL_CELLULES[0]):\n",
    "            for j in range(MODEL_CELLULES[1]):\n",
    "                for anchor in range(len(MODEL_ANCHOR_BOXES)):\n",
    "                    box = y_true[i, j, anchor]\n",
    "                    if box[4] == 1:\n",
    "                        # Coordonnées absolues de la boîte englobante dans l'image\n",
    "                        x_center_abs = (j + box[0]) * MODEL_CELLULES_SIZE[0]\n",
    "                        y_center_abs = (i + box[1]) * MODEL_CELLULES_SIZE[1]\n",
    "                        width_abs = box[2] * MODEL_ANCHOR_BOXES_PIXELSIZE[anchor][1]\n",
    "                        height_abs = box[3] * MODEL_ANCHOR_BOXES_PIXELSIZE[anchor][0]\n",
    "\n",
    "                        # Dessiner la boîte englobante\n",
    "                        x_min = x_center_abs - width_abs / 2\n",
    "                        y_min = y_center_abs - height_abs / 2\n",
    "                        rect = patches.Rectangle((x_min, y_min), width_abs, height_abs, linewidth=1, edgecolor=MODEL_ANCHOR_BOXES_COLOR[anchor], facecolor='none')\n",
    "                        ax.add_patch(rect)\n",
    "                        # Dessiner un point rouge au centre de la boîte englobante\n",
    "                        ax.scatter(x_center_abs, y_center_abs, color=MODEL_ANCHOR_BOXES_COLOR[anchor], s=2)  # `s` contrôle la taille du point\n",
    "\n",
    "    plt.title(f\"{nb_anchor}       {distribution_anchor_conf}\")\n",
    "    ax.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "# Fonction pour afficher un tensor 3D en 2D\n",
    "def show_tensor_nd(name, tensor, norm=False):\n",
    "    height, width, channels = tensor.shape\n",
    "    # Étendre les cellules horizontalement pour placer les valeurs de canaux côte à côte\n",
    "    # Création d'une image agrandie où chaque cellule est remplacée par une grille de N sous-cellules\n",
    "    decomposed_image = np.zeros((height, width * channels, 3))\n",
    "\n",
    "    for i in range(channels):\n",
    "        # Sélection des valeurs du canal i\n",
    "        channel_values = tensor[:, :, i]\n",
    "        # Normalisation des valeurs pour la cartographie de couleurs\n",
    "        if norm :\n",
    "            normalized_values = (channel_values - tf.reduce_min(channel_values)) / (tf.reduce_max(channel_values) - tf.reduce_min(channel_values))\n",
    "        else :\n",
    "            normalized_values = channel_values\n",
    "        # Mappage des valeurs normalisées à une carte de couleurs\n",
    "        cmap = plt.get_cmap('viridis')\n",
    "        colored_values = cmap(normalized_values.numpy())[:, :, :3]  # Prendre seulement les composantes RGB, ignorer alpha\n",
    "        # Remplir l'image agrandie, chaque canal placé côte à côte\n",
    "        decomposed_image[:, i::channels, :] = colored_values\n",
    "\n",
    "    # Affichage de l'image résultante\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    ax.imshow(decomposed_image, aspect='auto')\n",
    "\n",
    "    # Ajouter des lignes pour délimiter les canaux et les cellules\n",
    "    for y in range(1, height):\n",
    "        ax.axhline(y=y - 0.5, color='white', linestyle='-', linewidth=1)\n",
    "    for x in range(channels, width * channels, channels):\n",
    "        ax.axvline(x=x - 0.5, color='white', linestyle='-', linewidth=1)\n",
    "\n",
    "    # Ajouter une barre de couleur pour le gradient utilisé\n",
    "    norm = plt.Normalize(vmin=0, vmax=1)\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "    cbar.set_label('Normalized Channel Intensity')\n",
    "\n",
    "    # Configuration des titres et axes\n",
    "    ax.set_title(name)\n",
    "    ax.axis('off')  # Désactiver les axes pour une meilleure clarté\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "'''\n",
    "STATS\n",
    "'''\n",
    "print(f'Number of images in the dataset : {len(dataset_indices)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer les bounding boxes\n",
    "bounding_boxes = []\n",
    "for label,image in zip(os.listdir(LABELS_FOLDER), os.listdir(IMAGES_FOLDER)):\n",
    "    with open(os.path.join(LABELS_FOLDER, label), 'r') as f:\n",
    "        for line in f:\n",
    "            _, _, _, w, h = line.split()\n",
    "            # On récupère la taille de l'image dans le dossier images\n",
    "            with Image.open(os.path.join(IMAGES_FOLDER, image)) as img:\n",
    "                img_w, img_h = img.size\n",
    "            old_aspect_ratio = img_w / img_h\n",
    "\n",
    "            # Convertir les coordonnées pour les avoir en pixels puis en taille de cellules\n",
    "            w = float(w) * MODEL_CELLULES[1]\n",
    "            h = float(h) * MODEL_CELLULES[0]\n",
    "            # Ajouter la bounding box à la liste\n",
    "            bounding_boxes.append((w, h))\n",
    "\n",
    "# Convertir la liste en un numpy array pour l'utilisation avec scikit-learn\n",
    "kmean_data = np.array(bounding_boxes)\n",
    "print(f'Number of items in the dataset : {kmean_data.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k = 9 # Number of clusters\n",
    "\n",
    "# Initialiser k-means avec le nombre de clusters désiré\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "\n",
    "# Ajuster le modèle sur les données des boîtes englobantes\n",
    "kmeans.fit(kmean_data)\n",
    "\n",
    "# Les centres des clusters représentent vos anchor boxes optimales\n",
    "anchor_boxes = kmeans.cluster_centers_\n",
    "\n",
    "# Afficher les boîtes englobantes\n",
    "plt.scatter(kmean_data[:, 0], kmean_data[:, 1], c=kmeans.labels_, cmap='viridis', marker='o', label='Bounding box')\n",
    "\n",
    "# Afficher les centres des clusters (anchor boxes)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x', label='Cluster center')\n",
    "\n",
    "# Ajouter des titres et légendes\n",
    "plt.title('Anchor Boxes Clustering')\n",
    "plt.xlabel('Width')\n",
    "plt.ylabel('Height')\n",
    "plt.legend()\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.show()\n",
    "\n",
    "# Calculer et afficher les métriques\n",
    "inertia = kmeans.inertia_\n",
    "silhouette_avg = silhouette_score(kmean_data, kmeans.labels_)\n",
    "print(f'Model Inertia: {inertia}')\n",
    "print(f'Silhouette Score: {silhouette_avg}')\n",
    "print(f'Anchor boxes : {kmeans.cluster_centers_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class CustomDatasetLoaderYOLO(Sequence):\n",
    "    def __init__(self, subset):\n",
    "        def get_subset(subset, lenght):\n",
    "            train_end = int(DATASET_SPLIT[0] * lenght)\n",
    "            val_end = train_end + int(lenght* DATASET_SPLIT[1])\n",
    "            if subset == 'train':\n",
    "                return 0, train_end\n",
    "            elif subset == 'val':\n",
    "                return train_end,val_end\n",
    "            elif subset == 'test':\n",
    "                return val_end, lenght\n",
    "            else:\n",
    "                raise ValueError('Unknown subset (train, val, test)')\n",
    "        \n",
    "        start, stop = get_subset(subset, len(dataset_indices))\n",
    "        self.indices = dataset_indices[start:stop]\n",
    " \n",
    "    def __len__(self):\n",
    "        return np.ceil(len(self.indices) / MODEL_BATCH_SIZE).astype('int')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx * MODEL_BATCH_SIZE : (idx + 1) * MODEL_BATCH_SIZE]\n",
    "\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "        for image, label in dataset_filepath[batch_indices] :\n",
    "            # load image\n",
    "            with Image.open(os.path.join(IMAGES_FOLDER,image)) as img:\n",
    "                img = img.convert('RGB')\n",
    "                #original_image_height, original_image_width = img.size\n",
    "                #ratiox = original_image_height / IMAGE_SIZE[0]\n",
    "                #ratioy = original_image_width / IMAGE_SIZE[1]\n",
    "                img = img.resize((IMAGE_SIZE[1],IMAGE_SIZE[0]))\n",
    "                batch_images.append(np.array(img))\n",
    "            # load label\n",
    "            img_label = np.zeros((MODEL_CELLULES[0], MODEL_CELLULES[1], len(MODEL_ANCHOR_BOXES) ,5), dtype=np.float32)\n",
    "            for line in open(os.path.join(LABELS_FOLDER,label)).readlines():\n",
    "                _ ,x_center, y_center, width, height = line.split()[0:5] # we use _ because we don't use the class index\n",
    "                x_center, y_center, width, height = map(float, (x_center, y_center, width, height))\n",
    "                \n",
    "                \n",
    "                # Find the right cell\n",
    "                cell_x, rest_x = divmod(MODEL_CELLULES[1]*x_center, 1) # Y is height et X is width\n",
    "                cell_y, rest_y = divmod(MODEL_CELLULES[0]*y_center, 1)\n",
    "\n",
    "                # Find ther cell size\n",
    "                box_width, box_height = width * IMAGE_SIZE[1], height * IMAGE_SIZE[0]\n",
    "\n",
    "                # Find the right anchor box = the one with the best IOU\n",
    "                best_iou = 0\n",
    "                for i, (anchorbox_height, anchorbox_width) in enumerate(MODEL_ANCHOR_BOXES_PIXELSIZE):\n",
    "                    iou = min(anchorbox_width, box_width) * min(anchorbox_height, box_height) / (max(anchorbox_width, box_width) * max(anchorbox_height, box_height))\n",
    "                    if iou > best_iou:\n",
    "                        best_iou = iou\n",
    "                        best_anchor = i\n",
    "                # Calculate the coords inside the cell\n",
    "                width_cell = box_width / (MODEL_ANCHOR_BOXES[best_anchor][0]*MODEL_CELLULES_SIZE[0])\n",
    "                height_cell = box_height / (MODEL_ANCHOR_BOXES[best_anchor][1]*MODEL_CELLULES_SIZE[1])\n",
    "\n",
    "                # Adding coords to the label\n",
    "                img_label[int(cell_y), int(cell_x), best_anchor, 0] = rest_x\n",
    "                img_label[int(cell_y), int(cell_x), best_anchor, 1] = rest_y\n",
    "                img_label[int(cell_y), int(cell_x), best_anchor, 2] = width_cell\n",
    "                img_label[int(cell_y), int(cell_x), best_anchor, 3] = height_cell\n",
    "                img_label[int(cell_y), int(cell_x), best_anchor, 4] = 1 # Show that there is an item at this cell and anchor\n",
    "            batch_labels.append(img_label)\n",
    "\n",
    "        batch_images = np.asarray(batch_images) / 255\n",
    "        batch_labels = np.asarray(batch_labels).astype(np.float32)\n",
    "        \n",
    "        return batch_images ,batch_labels\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "        \n",
    "train_generator = CustomDatasetLoaderYOLO('train')\n",
    "val_generator = CustomDatasetLoaderYOLO('val')\n",
    "test_generator = CustomDatasetLoaderYOLO('test')\n",
    "\n",
    "print(f'train dataset lenght : {len(train_generator)}')\n",
    "print(f'Number of images in train dataset : {len(train_generator.indices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_image_with_boxes_and_grid(generator, figsize=(12, 12)):\n",
    "    # Extract data from generator\n",
    "    generator_len = len(generator)\n",
    "    random_id = np.random.randint(0, generator_len)\n",
    "    images, labels = generator[random_id]\n",
    "    idx = np.random.choice(images.shape[0])\n",
    "    image = images[idx]\n",
    "    fig, ax = plt.subplots(1, figsize=figsize)\n",
    "    # Show image\n",
    "    ax.imshow(image)\n",
    "\n",
    "\n",
    "    # Plot cells\n",
    "    for i in range(MODEL_CELLULES[1] + 1):  # Lignes verticales\n",
    "        ax.axvline(x=i * MODEL_CELLULES_SIZE[1], color='w', linestyle='-', linewidth=0.1)\n",
    "    for j in range(MODEL_CELLULES[0] + 1):  # Lignes horizontales\n",
    "        ax.axhline(y=j * MODEL_CELLULES_SIZE[0], color='w', linestyle='-', linewidth=0.1)\n",
    "\n",
    "    # Plot bounding boxes\n",
    "    label = labels[idx]\n",
    "    for i in range(MODEL_CELLULES[0]): \n",
    "        for j in range(MODEL_CELLULES[1]):\n",
    "            for anchor in range(len(MODEL_ANCHOR_BOXES_PIXELSIZE)):\n",
    "                box = label[i, j, anchor]\n",
    "                if box[4] == 1:  # Detect item\n",
    "                    # Absolute coords of the bounding box\n",
    "                    x_center_abs = (j + box[0]) * MODEL_CELLULES_SIZE[0]\n",
    "                    y_center_abs = (i + box[1]) * MODEL_CELLULES_SIZE[1]\n",
    "                    width_abs = box[2] * MODEL_ANCHOR_BOXES_PIXELSIZE[anchor][1]\n",
    "                    height_abs = box[3] * MODEL_ANCHOR_BOXES_PIXELSIZE[anchor][0]\n",
    "\n",
    "                    # Plot of the bounding box\n",
    "                    x_min = x_center_abs - width_abs / 2\n",
    "                    y_min = y_center_abs - height_abs / 2\n",
    "                    rect = patches.Rectangle((x_min, y_min), width_abs, height_abs, linewidth=1, edgecolor=MODEL_ANCHOR_BOXES_COLOR[anchor], facecolor='none')\n",
    "                    ax.add_patch(rect)\n",
    "                    ax.scatter(x_center_abs, y_center_abs, color='r', s=1)  # add little dot\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "show_sample_image_with_boxes_and_grid(select_generator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def YOLO():\n",
    "    def res_block(input, filters, size=2, kernel_size=3) :\n",
    "        skip = layers.Conv2D(filters, (1,1), padding='same')(input)\n",
    "        skip = layers.BatchNormalization()(skip)\n",
    "        skip = layers.LeakyReLU(negative_slope=0.1)(skip)\n",
    "        for i in range(size) :\n",
    "            input = layers.Conv2D(filters, kernel_size, padding='same')(input)\n",
    "            input = layers.BatchNormalization()(input)\n",
    "            input = layers.LeakyReLU(negative_slope=0.1)(input)\n",
    "        input = layers.Add()([skip, input])\n",
    "        input = layers.BatchNormalization()(input)\n",
    "        input = layers.LeakyReLU(negative_slope=0.1)(input)\n",
    "        return input\n",
    "\n",
    "    # Entrées\n",
    "    input_img = layers.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "    x = layers.BatchNormalization()(input_img)\n",
    "    x = res_block(x, 64, 3, kernel_size=5)\n",
    "    x = layers.Conv2D(64, 3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(negative_slope=0.1)(x)\n",
    "    x = res_block(x, 128, 3)\n",
    "    x = res_block(x, 128, 3)\n",
    "    x = layers.Conv2D(128, 3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(negative_slope=0.1)(x)\n",
    "    x = res_block(x, 256, 3)\n",
    "    x = res_block(x, 256, 3)\n",
    "    x = layers.Conv2D(256, 3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(negative_slope=0.1)(x)\n",
    "    x = res_block(x, 256, 3)\n",
    "    x = res_block(x, 256, 3)\n",
    "    x = layers.Conv2D(256, 3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(negative_slope=0.1)(x)\n",
    "    x = res_block(x, 512, 3)\n",
    "    x = res_block(x, 512, 3)\n",
    "    x = layers.Conv2D(512, 3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(negative_slope=0.1)(x)\n",
    "    x = res_block(x, 1024, 2)\n",
    "\n",
    "    # Sorties\n",
    "    output = layers.Conv2D(5*len(MODEL_ANCHOR_BOXES), (3, 3), padding='same')(x)\n",
    "    output = layers.Reshape((MODEL_CELLULES[0], MODEL_CELLULES[1], len(MODEL_ANCHOR_BOXES), 5))(output)\n",
    "\n",
    "    # Modèle\n",
    "    model = models.Model(inputs=input_img, outputs=output, name='YOLO')\n",
    "    return model\n",
    "\n",
    "def MiniYOLO():\n",
    "    def res_block(input, filters, size=2, kernel_size=3) :\n",
    "        skip = layers.Conv2D(filters, (1,1), padding='same')(input)\n",
    "        skip = layers.BatchNormalization()(skip)\n",
    "        skip = layers.LeakyReLU(negative_slope=0.1)(skip)\n",
    "        for i in range(size) :\n",
    "            input = layers.Conv2D(filters, kernel_size, padding='same')(input)\n",
    "            input = layers.BatchNormalization()(input)\n",
    "            input = layers.LeakyReLU(negative_slope=0.1)(input)\n",
    "        input = layers.Add()([skip, input])\n",
    "        input = layers.BatchNormalization()(input)\n",
    "        input = layers.LeakyReLU(negative_slope=0.1)(input)\n",
    "        return input\n",
    "\n",
    "    # Entrées\n",
    "    input_img = layers.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "    x = layers.BatchNormalization()(input_img)\n",
    "    x = res_block(x, 8, 1, kernel_size=5)\n",
    "    x = layers.Conv2D(8, 3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(negative_slope=0.1)(x)\n",
    "    x = res_block(x, 16, 1)\n",
    "    x = res_block(x, 16, 1)\n",
    "    x = layers.Conv2D(8, 3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(negative_slope=0.1)(x)\n",
    "    x = res_block(x, 32, 1)\n",
    "    x = res_block(x, 32, 1)\n",
    "    x = layers.Conv2D(16, 3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(negative_slope=0.1)(x)\n",
    "    x = res_block(x, 64, 1)\n",
    "    x = res_block(x, 64, 1)\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(negative_slope=0.1)(x)\n",
    "    x = res_block(x, 128, 1)\n",
    "    x = res_block(x, 128, 1)\n",
    "    x = layers.Conv2D(64, 3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(negative_slope=0.1)(x)\n",
    "    x = res_block(x, 256, 1)\n",
    "\n",
    "    # Sorties\n",
    "    output = layers.Conv2D(5*len(MODEL_ANCHOR_BOXES), (3, 3), padding='same')(x)\n",
    "    output = layers.Reshape((MODEL_CELLULES[0], MODEL_CELLULES[1], len(MODEL_ANCHOR_BOXES), 5))(output)\n",
    "\n",
    "    # Modèle\n",
    "    model = models.Model(inputs=input_img, outputs=output, name='MiniYOLO')\n",
    "    return model\n",
    "\n",
    "model = MiniYOLO()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIOU / PRED_IOU (loss_coord, loss_conf_iou, loss_noobj)\n",
    "#@tf.function\n",
    "def yolo_loss(y_true, y_pred):\n",
    "    #tensor_stats(\"y_pred\",y_pred)\n",
    "    #tensor_stats(\"y_true[0,3,27]\",y_true[0,3,27])\n",
    "    #tf.print(\"y_pred[0,3,27]\",y_pred[0,3,27])\n",
    "  \n",
    "    # Fonctions de perte de régression\n",
    "    binary_crossentropy = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    binary_crossentropy_ls = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE, label_smoothing=0.01)\n",
    "\n",
    "    # Définition de certains tenseurs constants\n",
    "    anchor_sizes = tf.constant(MODEL_ANCHOR_BOXES, dtype=tf.float32)\n",
    "    pattern_ascending = tf.tile(tf.range(MODEL_CELLULES[1], dtype=tf.float32)[tf.newaxis, :], [MODEL_CELLULES[0], 1]) # Pour les coordonnées x (0,1,2,..,MODEL_CELLULES[1]-1)\n",
    "    pattern_row_index = tf.tile(tf.range(MODEL_CELLULES[0], dtype=tf.float32)[:, tf.newaxis], [1, MODEL_CELLULES[1]]) # Pour les coordonnées y (0,0,0,..,MODEL_CELLULES[0]-1)\n",
    "    pattern_ascending = tf.expand_dims(pattern_ascending, axis=0)  # Ajouter des dimensions pour la compatibilité avec les calculs\n",
    "    pattern_ascending = tf.expand_dims(pattern_ascending, axis=-1)  # Ajouter des dimensions pour la compatibilité avec les calculs\n",
    "    pattern_row_index = tf.expand_dims(pattern_row_index, axis=0)  # Ajouter des dimensions pour la compatibilité avec les calculs\n",
    "    pattern_row_index = tf.expand_dims(pattern_row_index, axis=-1)  # Ajouter des dimensions pour la compatibilité avec les calculs\n",
    "\n",
    "    # Séparation des différentes parties des prédictions\n",
    "    pred_x, pred_y, pred_w, pred_h, pred_conf= tf.split(y_pred, (1, 1, 1, 1, 1), axis=-1)\n",
    "    pred_x = tf.squeeze(pred_x, axis=-1)\n",
    "    pred_y = tf.squeeze(pred_y, axis=-1)\n",
    "    pred_w = tf.squeeze(pred_w, axis=-1)\n",
    "    pred_h = tf.squeeze(pred_h, axis=-1)\n",
    "    pred_conf = tf.squeeze(pred_conf, axis=-1)\n",
    "    converted_pred_x = pattern_ascending + MODEL_GRID_SENSIBILITY_COEF*tf.sigmoid(pred_x) - (MODEL_GRID_SENSIBILITY_COEF-1)/2\n",
    "    converted_pred_y = pattern_row_index + MODEL_GRID_SENSIBILITY_COEF*tf.sigmoid(pred_y) - (MODEL_GRID_SENSIBILITY_COEF-1)/2\n",
    "    converted_pred_w = (MODEL_SIGMOID_MULTIPLIER*tf.sigmoid(pred_w) + MODEL_SIGMOID_ADDER )* anchor_sizes[:, 0]\n",
    "    converted_pred_h = (MODEL_SIGMOID_MULTIPLIER*tf.sigmoid(pred_h) + MODEL_SIGMOID_ADDER )* anchor_sizes[:, 1]\n",
    "    converted_pred_conf = tf.sigmoid(pred_conf)\n",
    "    \n",
    "    # Séparation des différentes parties des vérités terrain\n",
    "    true_x, true_y, true_w, true_h, true_conf = tf.split(y_true, (1, 1, 1, 1, 1), axis=-1)\n",
    "    true_x = tf.math.reduce_sum(tf.squeeze(true_x, axis=-1), axis=-1, keepdims=True)\n",
    "    true_y = tf.math.reduce_sum(tf.squeeze(true_y, axis=-1), axis=-1, keepdims=True)\n",
    "    converted_true_x = true_x + pattern_ascending\n",
    "    converted_true_y = true_y + pattern_row_index\n",
    "    #tensor_stats(\"converted_true_x\",converted_true_x)\n",
    "    #tensor_stats(\"converted_true_y\",converted_true_y)\n",
    "    true_w = tf.squeeze(true_w, axis=-1)\n",
    "    true_h = tf.squeeze(true_h, axis=-1)\n",
    "    converted_true_w = tf.math.reduce_sum(true_w * anchor_sizes[:, 0], axis=-1, keepdims=True)  # Largeur vraie ajustée\n",
    "    converted_true_h = tf.math.reduce_sum(true_h * anchor_sizes[:, 1], axis=-1, keepdims=True)  # Hauteur vraie ajustée\n",
    "    #tensor_stats(\"converted_true_w\",converted_true_w)\n",
    "    #tensor_stats(\"converted_true_h\",converted_true_h)\n",
    "    true_conf = tf.squeeze(true_conf, axis=-1)\n",
    "    #tensor_stats(\"true_conf\",true_conf)\n",
    "\n",
    "    # Masques pour les cellules avec et sans objet\n",
    "    obj = tf.cast(true_conf == 1, tf.float32)\n",
    "    obj_area = tf.reduce_sum(obj, axis=-1, keepdims=True)\n",
    "    noobj_area = tf.reduce_sum(1-obj, axis=-1, keepdims=True)\n",
    "    #show_tensor_nd(\"obj_area[0]\",obj_area[0])\n",
    "    noobj = tf.cast(true_conf == 0, tf.float32)\n",
    "    nb_obj = tf.reduce_sum(obj)\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Calcule la Distance Intersection sur Union (DIoU) entre les boîtes englobantes prédites et les vérités terrain,\n",
    "    en prenant en compte des tailles d'anchor boxes différentes.\n",
    "    \"\"\"\n",
    "    # Calcul des coins des boîtes\n",
    "    true_x_min, true_y_min = converted_true_x - converted_true_w / 2, converted_true_y - converted_true_h / 2\n",
    "    true_x_max, true_y_max = converted_true_x + converted_true_w / 2, converted_true_y + converted_true_h / 2\n",
    "        \n",
    "    pred_x_min, pred_y_min = converted_pred_x - converted_pred_w / 2, converted_pred_y - converted_pred_h / 2\n",
    "    pred_x_max, pred_y_max = converted_pred_x + converted_pred_w / 2, converted_pred_y + converted_pred_h / 2\n",
    "        \n",
    "    # Calcul des coordonnées des unions\n",
    "    inter_x_min = tf.maximum(true_x_min, pred_x_min)\n",
    "    inter_y_min = tf.maximum(true_y_min, pred_y_min)\n",
    "    inter_x_max = tf.minimum(true_x_max, pred_x_max)\n",
    "    inter_y_max = tf.minimum(true_y_max, pred_y_max)\n",
    "        \n",
    "    # Calcul des coordonnées des boxes englobantes (la plus petite box englobante couvrant les deux boxes)\n",
    "    englob_x_min = tf.minimum(true_x_min, pred_x_min)\n",
    "    englob_y_min = tf.minimum(true_y_min, pred_y_min)\n",
    "    englob_x_max = tf.maximum(true_x_max, pred_x_max)\n",
    "    englob_y_max = tf.maximum(true_y_max, pred_y_max)\n",
    "        \n",
    "    # Calcul de l'aire de l'intersection\n",
    "    inter_area = tf.maximum(inter_x_max - inter_x_min, 0) * tf.maximum(inter_y_max - inter_y_min, 0)\n",
    "    # Calcul d'aire des boîtes\n",
    "    true_area = (true_x_max - true_x_min) * (true_y_max - true_y_min)\n",
    "    pred_area = (pred_x_max - pred_x_min) * (pred_y_max - pred_y_min)\n",
    "    # Calcul de l'aire de l'union\n",
    "    union_area = true_area + pred_area - inter_area\n",
    "    # Calcul de la distance carré entre les centres\n",
    "    center_distance = tf.square(converted_true_x - converted_pred_x) + tf.square(converted_true_y - converted_pred_y)\n",
    "    #tf.print(\"center_distance[0,3,27]\",center_distance[0,3,27])\n",
    "    # Calcul de la \"smallest enclosing box covering the two boxes diagonal line squared\"\n",
    "    max_distance = tf.square( englob_x_max - englob_x_min) + tf.square(englob_y_max - englob_y_min)\n",
    "    max_box_area = (englob_x_max - englob_x_min) * (englob_y_max - englob_y_min)\n",
    "    #tf.print(\"max_distance[0,3,27]\",max_distance[0,3,27])\n",
    "    # Calcul de l'IOU\n",
    "    iou = inter_area / (union_area + 1e-6) # ATTENTION A LA DIVISION PAR ZERO\n",
    "    #tf.print(\"iou[0,3,27]\",iou[0,3,27])\n",
    "    # Calcul de la DIoU\n",
    "    distance_factor = center_distance / (max_distance + 1e-6) # ATTENTION A LA DIVISION PAR ZERO\n",
    "    #tf.print(\"max_distance[0,3,27]\",max_distance[0,3,27])\n",
    "    diou = iou - distance_factor\n",
    "    diouloss = (1 - diou) * obj_area\n",
    "    #tf.print(\"diouloss[0,3,27]\",diouloss[0,3,27])\n",
    "    # Calcul de la GIoU\n",
    "    giou = iou - (max_box_area - true_area - pred_area + union_area) / (max_box_area + 1e-6)\n",
    "    #tf.print(\"giou[0,3,27]\",giou[0,3,27])\n",
    "    giouloss = (1 - giou) * obj_area\n",
    "    #tf.print(\"giouloss[0,3,27]\",giouloss[0,3,27])\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    Calcul final de la perte\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calcul de la perte de coordonnées pour les objets\n",
    "    loss_coord = tf.reduce_sum(giouloss)\n",
    "    loss_coord = MODEL_LAMBDA_COORD * loss_coord / (nb_obj*len(MODEL_ANCHOR_BOXES)+1e-6)\n",
    "\n",
    "    \n",
    "    # Calcul de la perte de d'IOU (le but étant que le modèle apprenne à predite son IOU)\n",
    "    loss_pred_iou = MODEL_LAMBDA_IOU * tf.reduce_sum(binary_crossentropy(converted_pred_conf*obj_area, iou)) / (nb_obj+1e-6)\n",
    "\n",
    "    # Calcul de la perte de confiance pour les cellules sans objet\n",
    "    loss_pred_noobj = MODEL_LAMBDA_NOOBJ * tf.reduce_sum(binary_crossentropy_ls(converted_pred_conf*noobj_area, true_conf*noobj_area)) / (MODEL_BATCH_SIZE * (MODEL_CELLULES[0] * MODEL_CELLULES[1] * len(MODEL_ANCHOR_BOXES)) - nb_obj+1e-6)\n",
    "\n",
    "    # Calcul de la parte de confiance\n",
    "    total_loss = loss_coord + loss_pred_iou + loss_pred_noobj\n",
    "    \n",
    "    \n",
    "    #best_iou_mean = tf.reduce_sum(tf.reduce_max(iou, axis=-1)) / (nb_obj+1e-6)\n",
    "    \n",
    "    return total_loss, (loss_coord, loss_pred_iou, loss_pred_noobj)\n",
    "\n",
    "\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=MODEL_LEARNING_RATE)\n",
    "\n",
    "# Fonction d'entraînement personnalisée\n",
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss, loss_data = yolo_loss(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss, loss_data\n",
    "\n",
    "# Boucle d'entraînement principale\n",
    "best_loss_epoch = 0\n",
    "best_loss = float('inf')\n",
    "history = {\n",
    "    'loss' : [],\n",
    "    'loss_coord' : [],\n",
    "    'loss_pred_iou' : [],\n",
    "    'loss_pred_noobj' : [],\n",
    "}\n",
    "val_history = {\n",
    "    'val_loss' : [],\n",
    "    'val_loss_coord' : [],\n",
    "    'val_loss_pred_iou' : [],\n",
    "    'val_loss_pred_noobj' : [],\n",
    "}\n",
    "for epoch in range(MODEL_EPOCHS):\n",
    "    found_better_loss = False\n",
    "    # Train loop\n",
    "    with tqdm(total=len(train_generator), desc=f'Training {epoch+1}/{MODEL_EPOCHS}', unit='batch') as pbar:\n",
    "        epoch_losses = {\n",
    "            'loss' : [],\n",
    "            'loss_coord' : [],\n",
    "            'loss_pred_iou' : [],\n",
    "            'loss_pred_noobj' : [],\n",
    "        }\n",
    "        for step in range(len(train_generator)) :\n",
    "            inputs, labels = train_generator[step]\n",
    "            # Batch training\n",
    "            loss, loss_data = train_step(inputs, labels)\n",
    "            loss_coord, loss_pred_iou, loss_pred_noobj = loss_data\n",
    "            # Metrics\n",
    "            pbar.set_postfix({'Loss' :f\"{loss.numpy():.6f}\",\n",
    "                              'Loss coord' :f\"{loss_coord.numpy():.6f}\",\n",
    "                              'Loss pred iou' :f\"{loss_pred_iou.numpy():.6f}\",\n",
    "                              'Loss noobj' :f\"{loss_pred_noobj.numpy():.6f}\",\n",
    "            }\n",
    "            )\n",
    "            epoch_losses['loss'].append(loss.numpy())\n",
    "            epoch_losses['loss_coord'].append(loss_coord.numpy())\n",
    "            epoch_losses['loss_pred_iou'].append(loss_pred_iou.numpy())\n",
    "            epoch_losses['loss_pred_noobj'].append(loss_pred_noobj.numpy())\n",
    "            \n",
    "            pbar.update()\n",
    "    train_generator.on_epoch_end()\n",
    "    epoch_loss = np.mean(epoch_losses['loss'])\n",
    "    history['loss'].append(epoch_losses['loss'])\n",
    "    history['loss_coord'].append(epoch_losses['loss_coord'])\n",
    "    history['loss_pred_iou'].append(epoch_losses['loss_pred_iou'])\n",
    "    history['loss_pred_noobj'].append(epoch_losses['loss_pred_noobj'])\n",
    "\n",
    "    # Val loop\n",
    "    with tqdm(total=len(val_generator), desc=f'Validation {epoch+1}/{MODEL_EPOCHS}', unit='batch') as pbar:\n",
    "        val_epoch_losses = {\n",
    "            'val_loss' : [],\n",
    "            'val_loss_coord' : [],\n",
    "            'val_loss_pred_iou' : [],\n",
    "            'val_loss_pred_noobj' : [],\n",
    "        }\n",
    "        for step in range(len(val_generator)) :\n",
    "            inputs, labels = val_generator[step]\n",
    "            # Batch inference\n",
    "            predictions = model(inputs, training=True)\n",
    "            loss, loss_data = yolo_loss(labels, predictions)\n",
    "            loss_coord, loss_pred_iou, loss_pred_noobj = loss_data\n",
    "            # Metrics\n",
    "            pbar.set_postfix({'Loss' :f\"{loss.numpy():.6f}\",\n",
    "                              'Loss coord' :f\"{loss_coord.numpy():.6f}\",\n",
    "                              'Loss pred iou' :f\"{loss_pred_iou.numpy():.6f}\",\n",
    "                              'Loss noobj' :f\"{loss_pred_noobj.numpy():.6f}\",\n",
    "            }\n",
    "            )\n",
    "            val_epoch_losses['val_loss'].append(loss.numpy())\n",
    "            val_epoch_losses['val_loss_coord'].append(loss_coord.numpy())\n",
    "            val_epoch_losses['val_loss_pred_iou'].append(loss_pred_iou.numpy())\n",
    "            val_epoch_losses['val_loss_pred_noobj'].append(loss_pred_noobj.numpy())\n",
    "        \n",
    "            pbar.update()\n",
    "    val_history['val_loss'].append(val_epoch_losses['val_loss'])\n",
    "    val_history['val_loss_coord'].append(val_epoch_losses['val_loss_coord'])\n",
    "    val_history['val_loss_pred_iou'].append(val_epoch_losses['val_loss_pred_iou'])\n",
    "    val_history['val_loss_pred_noobj'].append(val_epoch_losses['val_loss_pred_noobj'])\n",
    "    \n",
    "    # Compare epoch with the best epoch yet\n",
    "    if epoch_loss < best_loss :\n",
    "        best_loss_epoch = epoch\n",
    "        best_loss = epoch_loss\n",
    "        found_better_loss = True\n",
    "\n",
    "    # Plot\n",
    "    if found_better_loss:\n",
    "        random_id = np.random.randint(0, len(val_generator))\n",
    "        selected_images, selected_labels = val_generator[random_id]\n",
    "        random_idx = np.random.randint(0, selected_images.shape[0])\n",
    "        selected_image = np.expand_dims(selected_images[random_idx], axis=0)\n",
    "        selected_label = selected_labels[random_idx]\n",
    "        # Inference\n",
    "        predictions = model(selected_image, training=True)[0]\n",
    "        show_tensor_nd(\"predictions_conf\",predictions[...,4])\n",
    "        draw_predict(selected_image[0], y_pred=predictions, nms=True) # Use NMS\n",
    "\n",
    "    # Early stopping\n",
    "    if epoch - best_loss_epoch >= MODEL_PATIENCE:\n",
    "        print(f\"Training stopped. No improvement was seen in the last {MODEL_PATIENCE} epochs.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour avoir des informations sur un tensor\n",
    "def tensor_stats(name,tensor, simple = False, print_tensor = False):\n",
    "    # Calcul de l'élément le plus petit\n",
    "    min_val = tf.reduce_min(tensor)\n",
    "        \n",
    "    # Calcul de l'élément le plus grand\n",
    "    max_val = tf.reduce_max(tensor)\n",
    "        \n",
    "    # Calcul de la moyenne des éléments\n",
    "    mean_val = tf.reduce_mean(tf.cast(tensor, tf.float32))\n",
    "        \n",
    "    # Calcul de l'écart-type des éléments\n",
    "    stddev_val = tf.math.reduce_std(tf.cast(tensor, tf.float32))\n",
    "        \n",
    "    # Calcul de la médiane des éléments\n",
    "    # Pour la médiane, nous devons d'abord aplatir le tensor, le trier, puis trouver l'élément médian\n",
    "    tensor_flat = tf.reshape(tensor, [-1])\n",
    "    tensor_sorted = tf.sort(tensor_flat)\n",
    "    median_val = tensor_sorted[tf.size(tensor_flat) // 2] if tf.size(tensor_flat) % 2 != 0 else \\\n",
    "                    (tensor_sorted[tf.size(tensor_flat) // 2 - 1] + tensor_sorted[tf.size(tensor_flat) // 2]) / 2.0\n",
    "        \n",
    "    # Utiliser tf.print pour afficher les résultats\n",
    "\n",
    "    if simple :\n",
    "        tf.print(name, \":\",'Value:', tensor)\n",
    "    else :\n",
    "        tf.print(name, \":\",'Min:', min_val, 'Max:', max_val, 'Mean:', mean_val, 'Stddev:', stddev_val, 'Median:', median_val, 'Shape:', tf.shape(tensor))\n",
    "    if print_tensor :\n",
    "        tf.print(tensor, summarize=-1)\n",
    "\n",
    "# Assumant une fonction de tracé généralisée pour toutes les métriques\n",
    "def plot_all_metrics(metrics, title='Training and Validation Metrics'):\n",
    "    epochs = range(1, len(next(iter(metrics.values()))) + 1)\n",
    "    num_metrics = len(metrics) // 2\n",
    "    print(\"num_metrics : \",num_metrics)\n",
    "\n",
    "    with plt.style.context('ggplot'):\n",
    "        plt.figure(figsize=(15, 5 * num_metrics))  # Augmenter la résolution\n",
    "\n",
    "        for i, (key, val_key) in enumerate([(k, f'val_{k}') for k in metrics if not k.startswith('val_')]):\n",
    "            print(f\"Plotting {key} and {val_key}\")  # Ajouter pour voir quels graphiques sont tracés\n",
    "            plt.subplot(num_metrics, 1, i + 1)\n",
    "            if len(metrics[key]) == 0 :\n",
    "                print(f\"Skipping {key} because of empty data len(metrics[key]) : {len(metrics[key])} len(metrics[val_key]) : {len(metrics[val_key])}\")\n",
    "            else :\n",
    "                plt.plot(epochs, metrics[key], label=f'Training {key}', color='blue')\n",
    "            if len(metrics[val_key]) == 0 :\n",
    "                print(f\"Skipping {val_key} because of empty data len(metrics[key]) : {len(metrics[key])} len(metrics[val_key]) : {len(metrics[val_key])}\")\n",
    "            else :\n",
    "                plt.plot(epochs, metrics[val_key], label=f'Validation {key}', color='orange')\n",
    "            if len(metrics[key]) != 0 or len(metrics[val_key]) != 0 :\n",
    "                plt.title(title)\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel(key)\n",
    "                plt.legend(loc='best')\n",
    "                plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Fonction de dessin des prédictions\n",
    "def draw_predict(image, y_pred=None, y_true=None, showprederrors=True, nms=False, only_true=False):\n",
    "    def sigmoid(x):\n",
    "        x=np.clip(x, -50, 50)\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    def softmax(x):\n",
    "        e=np.exp(x)\n",
    "        e_sum=np.sum(e)\n",
    "        return e/e_sum\n",
    "\n",
    "    if only_true and y_true is None:\n",
    "        print(\"Erreur : only_true est True mais y_true est None\")\n",
    "        return\n",
    "    fig, ax = plt.subplots(1)\n",
    "    # Séparer les prédictions\n",
    "    if y_pred is not None:\n",
    "        pred_boxes=y_pred[..., 0:4]\n",
    "        pred_conf=tf.sigmoid(y_pred[..., 4])\n",
    "        # Afficher l'image avec les bounding boxes\n",
    "        nb_anchor = 0\n",
    "        #distribution_anchor = [0 for i in range(len(MODEL_ANCHOR_BOXES))]\n",
    "        distribution_anchor_conf = [0 for i in range(len(MODEL_ANCHOR_BOXES))]\n",
    "        # Dessiner les lignes verticales et horizontales pour les cellules\n",
    "        for i in range(MODEL_CELLULES[1] + 1):  # Lignes verticales\n",
    "            ax.axvline(x=i * MODEL_CELLULES_SIZE[1], color='w', linestyle='-', linewidth=0.1)\n",
    "        for j in range(MODEL_CELLULES[0] + 1):  # Lignes horizontales\n",
    "            ax.axhline(y=j * MODEL_CELLULES_SIZE[0], color='w', linestyle='-', linewidth=0.1)\n",
    "        # Ajouter les prédictions des boîtes englobantes à l'image\n",
    "        for i in range(MODEL_CELLULES[0]):\n",
    "            for j in range(MODEL_CELLULES[1]):\n",
    "                show_box = True\n",
    "                if only_true :\n",
    "                    show_box = False if np.sum(y_true[i,j,:,4]) == 0 else True\n",
    "                if nms and show_box :\n",
    "                    k_max = np.argmax(pred_conf[i, j])\n",
    "                    box = pred_boxes[i, j, k_max]\n",
    "                    if pred_conf[i,j,k_max] > THRESHOLD_CONFIDENCE :\n",
    "                        # Conversion de la box prédite en coordonnées\n",
    "                        x_center = (j + MODEL_GRID_SENSIBILITY_COEF*sigmoid(box[0]) - (MODEL_GRID_SENSIBILITY_COEF-1)/2) * MODEL_CELLULES_SIZE[1]\n",
    "                        y_center = (i + MODEL_GRID_SENSIBILITY_COEF*sigmoid(box[1]) - (MODEL_GRID_SENSIBILITY_COEF-1)/2) * MODEL_CELLULES_SIZE[0]\n",
    "                        w_box = (MODEL_SIGMOID_MULTIPLIER*sigmoid(box[2]) + MODEL_SIGMOID_ADDER ) * MODEL_ANCHOR_BOXES[k_max][0] * MODEL_CELLULES_SIZE[1]\n",
    "                        h_box = (MODEL_SIGMOID_MULTIPLIER*sigmoid(box[3]) + MODEL_SIGMOID_ADDER ) * MODEL_ANCHOR_BOXES[k_max][1] * MODEL_CELLULES_SIZE[0]\n",
    "                        # Calcul des coins de la box\n",
    "                        x_min = x_center - w_box / 2\n",
    "                        y_min = y_center - h_box / 2\n",
    "                        # Ajout des données à la distribution\n",
    "                        nb_anchor += 1\n",
    "                        distribution_anchor_conf[k_max] += 1\n",
    "                        # Dessiner la boîte englobante\n",
    "                        rect=patches.Rectangle((x_min, y_min), w_box, h_box, linewidth=1, edgecolor=MODEL_ANCHOR_BOXES_COLOR[k_max], facecolor='none')\n",
    "                        ax.add_patch(rect)\n",
    "                        ax.scatter(x_center, y_center, color=MODEL_ANCHOR_BOXES_COLOR[k_max], s=2)\n",
    "                elif show_box :\n",
    "                    for k in range(len(MODEL_ANCHOR_BOXES)):\n",
    "                        box = pred_boxes[i, j, k]\n",
    "                        if pred_conf[i, j, k] > THRESHOLD_CONFIDENCE :\n",
    "                            # Conversion de la box prédite en coordonnées\n",
    "                            x_center = (j + MODEL_GRID_SENSIBILITY_COEF*sigmoid(box[0]) - (MODEL_GRID_SENSIBILITY_COEF-1)/2) * MODEL_CELLULES_SIZE[1]\n",
    "                            y_center = (i + MODEL_GRID_SENSIBILITY_COEF*sigmoid(box[1]) - (MODEL_GRID_SENSIBILITY_COEF-1)/2) * MODEL_CELLULES_SIZE[0]\n",
    "                            #w_box = np.exp(box[2]) * MODEL_ANCHOR_BOXES[k][0] * MODEL_CELLULES_SIZE[1] # A AMELIORER\n",
    "                            w_box = (MODEL_SIGMOID_MULTIPLIER*sigmoid(box[2]) + MODEL_SIGMOID_ADDER )* MODEL_ANCHOR_BOXES[k][0] * MODEL_CELLULES_SIZE[1]\n",
    "                            #h_box = np.exp(box[3]) * MODEL_ANCHOR_BOXES[k][1] * MODEL_CELLULES_SIZE[0] # A AMELIORER\n",
    "                            h_box = (MODEL_SIGMOID_MULTIPLIER*sigmoid(box[3]) + MODEL_SIGMOID_ADDER )* MODEL_ANCHOR_BOXES[k][1] * MODEL_CELLULES_SIZE[0]\n",
    "                                \n",
    "                            # Calcul des coins de la box\n",
    "                            x_min = x_center - w_box / 2\n",
    "                            y_min = y_center - h_box / 2\n",
    "                            # Ajout des données à la distribution\n",
    "                            nb_anchor += 1\n",
    "                            distribution_anchor_conf[k] += 1\n",
    "                            # Dessiner la boîte englobante\n",
    "                            rect=patches.Rectangle((x_min, y_min), w_box, h_box, linewidth=1, edgecolor=MODEL_ANCHOR_BOXES_COLOR[k], facecolor='none')\n",
    "                            ax.add_patch(rect)\n",
    "                            ax.scatter(x_center, y_center, color=MODEL_ANCHOR_BOXES_COLOR[k], s=2)\n",
    "\n",
    "    # Ajouter les vérités terrain à l'image\n",
    "    if y_true is not None and not only_true:\n",
    "        # On converti les coordonnées des vérités terrain\n",
    "        for i in range(MODEL_CELLULES[0]):\n",
    "            for j in range(MODEL_CELLULES[1]):\n",
    "                for anchor in range(len(MODEL_ANCHOR_BOXES)):\n",
    "                    box = y_true[i, j, anchor]\n",
    "                    if box[4] == 1:\n",
    "                        # Coordonnées absolues de la boîte englobante dans l'image\n",
    "                        x_center_abs = (j + box[0]) * MODEL_CELLULES_SIZE[0]\n",
    "                        y_center_abs = (i + box[1]) * MODEL_CELLULES_SIZE[1]\n",
    "                        width_abs = box[2] * MODEL_ANCHOR_BOXES_PIXELSIZE[anchor][1]\n",
    "                        height_abs = box[3] * MODEL_ANCHOR_BOXES_PIXELSIZE[anchor][0]\n",
    "\n",
    "                        # Dessiner la boîte englobante\n",
    "                        x_min = x_center_abs - width_abs / 2\n",
    "                        y_min = y_center_abs - height_abs / 2\n",
    "                        rect = patches.Rectangle((x_min, y_min), width_abs, height_abs, linewidth=1, edgecolor=MODEL_ANCHOR_BOXES_COLOR[anchor], facecolor='none')\n",
    "                        ax.add_patch(rect)\n",
    "                        # Dessiner un point rouge au centre de la boîte englobante\n",
    "                        ax.scatter(x_center_abs, y_center_abs, color=MODEL_ANCHOR_BOXES_COLOR[anchor], s=2)  # `s` contrôle la taille du point\n",
    "\n",
    "    plt.title(f\"{nb_anchor}       {distribution_anchor_conf}\")\n",
    "    ax.imshow(image)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
