{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdd8adca",
   "metadata": {},
   "source": [
    "## CycleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d2d6e7",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53169688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, losses, callbacks\n",
    "from tensorflow.keras.models import save_model, load_model, Model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.utils import plot_model, Sequence\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import random as r\n",
    "from tqdm import tqdm # progress bar\n",
    "from IPython.display import clear_output\n",
    "import seaborn as sns\n",
    "import math\n",
    "from tqdm import trange\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import pandas as pd      # si tu l’as déjà dans l’environnement\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPUs detected: {len(gpus)}\")\n",
    "        print(f\"GPUs: {gpus}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPUs detected\")\n",
    "\n",
    "tf.config.optimizer.set_jit(True)  # Active JIT (XLA) globalement\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "r.seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7684f432",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2c4c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_lr, min_lr, total_iteration):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.iteration = 0\n",
    "        self.total_iteration = total_iteration\n",
    "\n",
    "    def __call__(self, step):\n",
    "        lr = self.min_lr + (self.initial_lr - self.min_lr) * ( 1 - ( self.iteration / self.total_iteration ) )\n",
    "        self.iteration += 1\n",
    "        return lr\n",
    "\n",
    "\n",
    "# Constants\n",
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "BATCH_SIZE = 1\n",
    "TOTAL_ITERATIONS = 200000\n",
    "initial_lr = 2e-4\n",
    "min_lr = 5e-6\n",
    "COEF_AV, COEF_CYC, COEF_ID = 1, 10, 5 # Coefficients de la loss du générateur\n",
    "RATIO_GEN, RATIO_DISC = 3 , 1 # Nombre de fois que le générateur est mis à jour par rapport au discriminateur\n",
    "TRAINING_CYCLE = ['generator']*RATIO_GEN + ['discriminator']*RATIO_DISC\n",
    "\n",
    "gen_lr_schedule = LinearScheduler(initial_lr, min_lr, TOTAL_ITERATIONS)\n",
    "disc_lr_schedule = LinearScheduler(initial_lr, min_lr, TOTAL_ITERATIONS)\n",
    "\n",
    "GENERATOR_OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=gen_lr_schedule, beta_1=0.5)\n",
    "DISCRIMINATOR_OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=disc_lr_schedule, beta_1=0.5)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "PATIENCE = 2\n",
    "\n",
    "def smooth(series, span=100):\n",
    "    \"\"\"EMA lissée (span≈ longueur de fenêtre).\"\"\"\n",
    "    return pd.Series(series).ewm(span=span, adjust=False).mean().values\n",
    "\n",
    "def plot_history(history, span=100):\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    for key, value in history.items():\n",
    "        value = smooth(value, span=span)\n",
    "        plt.plot(value, label=key)\n",
    "    plt.title('Training History')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_inference(image=None):\n",
    "    result = inference(for_plot=True, image=image)\n",
    "    fig, axes = plt.subplots(1, len(result), figsize=(20, 5))\n",
    "    for i, res in enumerate(result):\n",
    "        step, img = res\n",
    "        img = (img + 1) / 2 # Convertion de -1, 1 à 0, 1\n",
    "        axes[i].imshow(np.clip(img, 0, 1))\n",
    "        axes[i].set_title(f'{int(step)}')\n",
    "        axes[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def decode_jpeg(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)  # [0,1]\n",
    "    return img\n",
    "\n",
    "def augment(img):\n",
    "    img = tf.image.resize(img, [286, 286])\n",
    "    img = tf.image.random_crop(img, [256, 256, 3])\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9599ad79",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b7f3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summer <-> Winter\n",
    "def build_dataset_wintersummer(ensemble, cache_to_ram=True):\n",
    "    if ensemble == 'train':\n",
    "        summer_dir = 'datasets/summer_winter/train_summer'\n",
    "        winter_dir = 'datasets/summer_winter/train_winter'\n",
    "    elif ensemble == 'test':\n",
    "        summer_dir = 'datasets/summer_winter/test_summer'\n",
    "        winter_dir = 'datasets/summer_winter/test_winter'\n",
    "    else:\n",
    "        raise ValueError(\"set must be either 'train' or 'test'\")\n",
    "    summer_files = os.listdir(summer_dir)\n",
    "    winter_files = os.listdir(winter_dir)\n",
    "\n",
    "    summer_files = [os.path.join(summer_dir, file) for file in summer_files]\n",
    "    winter_files = [os.path.join(winter_dir, file) for file in winter_files]\n",
    "    # Summer\n",
    "    summer_ds = tf.data.Dataset.from_tensor_slices(summer_files)\n",
    "    summer_ds = summer_ds.shuffle(len(summer_files))                      # mélange global\n",
    "    summer_ds = summer_ds.map(decode_jpeg, num_parallel_calls=AUTOTUNE)\n",
    "    summer_ds = summer_ds.map(augment,       num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    # Winter\n",
    "    winter_ds = tf.data.Dataset.from_tensor_slices(winter_files)\n",
    "    winter_ds = winter_ds.shuffle(len(winter_files))                      # mélange global\n",
    "    winter_ds = winter_ds.map(decode_jpeg, num_parallel_calls=AUTOTUNE)\n",
    "    winter_ds = winter_ds.map(augment,       num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    if cache_to_ram:\n",
    "        summer_ds = summer_ds.cache()\n",
    "        winter_ds = winter_ds.cache()\n",
    "    else:\n",
    "        summer_ds = summer_ds.cache(\"summer_winter.cache\")             # ou sur disque\n",
    "        winter_ds = winter_ds.cache(\"winter_summer.cache\")             # ou sur disque\n",
    "    \n",
    "    summer_ds = summer_ds.repeat()                                     # boucle infinie\n",
    "    summer_ds = summer_ds.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    summer_ds = summer_ds.prefetch(AUTOTUNE)\n",
    "    \n",
    "    winter_ds = winter_ds.repeat()                                     # boucle infinie\n",
    "    winter_ds = winter_ds.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    winter_ds = winter_ds.prefetch(AUTOTUNE)\n",
    "\n",
    "    ds = tf.data.Dataset.zip((summer_ds, winter_ds))\n",
    "    return iter(ds)\n",
    "\n",
    "TrainGen = build_dataset_wintersummer('train', cache_to_ram=True)\n",
    "TestGen = build_dataset_wintersummer('test', cache_to_ram=True)\n",
    "\n",
    "# Visualisation des données\n",
    "def plot_data_gen(gen, n=5):\n",
    "    fig, axes = plt.subplots(2, n, figsize=(20, 5))\n",
    "    for i in range(n):\n",
    "        image_A, image_B = next(gen)\n",
    "        axes[0][i].imshow(image_A[0])\n",
    "        axes[0][i].axis('off')\n",
    "        axes[0, i].set_title(\"A\")\n",
    "        axes[1][i].imshow(image_B[0])\n",
    "        axes[1][i].axis('off')\n",
    "        axes[1, i].set_title(\"B\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_data_gen(TrainGen, n=5)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b96186d",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5421e37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, filters, kernel_size=3, strides=1, padding='same', activation='leaky_relu', norm=True, dropout=0.0):\n",
    "    x = layers.Conv2D(filters, kernel_size=kernel_size, strides=strides, padding=padding)(x)\n",
    "    if norm:\n",
    "        x = layers.BatchNormalization()(x) # InstanceNormalization n'est pas dispo avec tf 2.10\n",
    "    if activation:\n",
    "        x = layers.Activation(activation)(x)\n",
    "    if dropout > 0.0:\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "    return x\n",
    "\n",
    "def residual_block(x, nb, filters, kernel_size=3, strides=1, padding='same', activation='leaky_relu', norm=True, dropout=0.0):\n",
    "    # ResNet block\n",
    "    skip = x\n",
    "    for i in range(nb):\n",
    "        x = conv_block(x, filters, kernel_size, strides, padding, activation, norm, dropout)\n",
    "    # Add skip connection\n",
    "    x = layers.Add()([x, skip])\n",
    "    if norm:\n",
    "        x = layers.BatchNormalization()(x) # InstanceNormalization n'est pas dispo avec tf 2.10\n",
    "    if activation:\n",
    "        x = layers.Activation(activation)(x)\n",
    "    if dropout > 0.0:\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_GeneratorModel(name=\"Generator\"):\n",
    "    image_input = layers.Input(shape=(IMAGE_SHAPE))\n",
    "\n",
    "    # DOWN\n",
    "    d1 = conv_block(image_input, 64, kernel_size=3, strides=2, padding='same', activation='leaky_relu', norm=True)\n",
    "    d1 = residual_block(d1, 2, 64, kernel_size=3, strides=1, padding='same', activation='leaky_relu', norm=True)\n",
    "    d1 = residual_block(d1, 2, 64, kernel_size=3, strides=1, padding='same', activation='leaky_relu', norm=True)\n",
    "    \n",
    "    d2 = conv_block(d1, 128, kernel_size=3, strides=2, padding='same', activation='leaky_relu', norm=True)\n",
    "    d2 = residual_block(d2, 2, 128, kernel_size=3, strides=1, padding='same', activation='leaky_relu', norm=True)\n",
    "    d2 = residual_block(d2, 2, 128, kernel_size=3, strides=1, padding='same', activation='leaky_relu', norm=True)\n",
    "    \n",
    "    # BOTTLENECK\n",
    "    d3 = conv_block(d2, 256, kernel_size=3, strides=2, padding='same', activation='leaky_relu', norm=True)\n",
    "    for i in range(9):\n",
    "        d3 = residual_block(d3, 2, 256, kernel_size=3, strides=1, padding='same', activation='leaky_relu', norm=True)\n",
    "    \n",
    "    # UP\n",
    "    u1 = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(d3)\n",
    "    u1 = layers.Concatenate()([u1, d2])\n",
    "    u1 = conv_block(u1, 128, kernel_size=3, strides=1, padding='same', activation='leaky_relu', norm=True)\n",
    "    u1 = residual_block(u1, 2, 128, kernel_size=3, strides=1, padding='same', activation='leaky_relu', norm=True)\n",
    "    u1 = residual_block(u1, 2, 128, kernel_size=3, strides=1, padding='same', activation='leaky_relu', norm=True)\n",
    "\n",
    "    u2 = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(u1)\n",
    "    u2 = layers.Concatenate()([u2, d1])\n",
    "    u2 = conv_block(u2, 64, kernel_size=3, strides=1, padding='same', activation='leaky_relu', norm=True)\n",
    "    u2 = residual_block(u2, 2, 64, kernel_size=3, strides=1, padding='same', activation='leaky_relu', norm=True)\n",
    "    u2 = residual_block(u2, 2, 64, kernel_size=3, strides=1, padding='same', activation='leaky_relu', norm=True)\n",
    "\n",
    "    u0 = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(u2)\n",
    "    u0 = layers.Concatenate()([u0, image_input])\n",
    "    u0 = conv_block(u0, 64, kernel_size=3, strides=1, padding='same', activation='leaky_relu', norm=True)\n",
    "    u0 = residual_block(u0, 2, 64, kernel_size=3, strides=1, padding='same', activation='leaky_relu', norm=True)\n",
    "    u0 = residual_block(u0, 2, 64, kernel_size=3, strides=1, padding='same', activation='leaky_relu', norm=True)\n",
    "\n",
    "    output = conv_block(u0, 3, kernel_size=3, strides=1, padding='same', activation='sigmoid', norm=False)\n",
    "    model = Model(inputs=image_input, outputs=output, name=name)\n",
    "    return model\n",
    "\n",
    "def build_DiscriminatorModel(name=\"Discriminator\"):\n",
    "    \"\"\"\n",
    "    Discriminateur de type PatchGAN 70x70\n",
    "    \"\"\"\n",
    "    image_input = layers.Input(shape=IMAGE_SHAPE)\n",
    "    x = conv_block(image_input, 64, kernel_size=7, strides=2)\n",
    "\n",
    "    x = conv_block(x, 128, kernel_size=4, strides=2)\n",
    "\n",
    "    x = conv_block(x, 256, kernel_size=4, strides=2)\n",
    "\n",
    "    x = conv_block(x, 512, kernel_size=4)\n",
    "\n",
    "    x = conv_block(x, 1, kernel_size=4, activation='sigmoid', norm=False) # Pas de BatchNorm à la fin sinon on tend vers 0.5 (loss_d à 1.38)\n",
    "    \n",
    "    return Model(inputs=image_input, outputs=x, name=name)\n",
    "\n",
    "GeneratorModel_A, DiscriminatorModel_A = build_GeneratorModel('GeneratorB2A'), build_DiscriminatorModel('DiscriminatorA')\n",
    "GeneratorModel_B, DiscriminatorModel_B = build_GeneratorModel('GeneratorA2B'), build_DiscriminatorModel('DiscriminatorB')\n",
    "print(GeneratorModel_A.summary())\n",
    "print(DiscriminatorModel_A.summary())\n",
    "print(GeneratorModel_B.summary())\n",
    "print(DiscriminatorModel_B.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e821fa",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e532b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)  # car output= sigmoid\n",
    "\n",
    "@tf.function(jit_compile=True)\n",
    "def train_step(images, Generators, Discriminators, turn):\n",
    "    gen_a, gen_b = Generators\n",
    "    disc_a, disc_b = Discriminators\n",
    "    images_a, images_b = images\n",
    "    total_g_loss, total_d_loss = None, None\n",
    "    loss_data = {}\n",
    "    # === Générateur ===\n",
    "    if 'generator' in turn :\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            # Transition dans les deux sens\n",
    "            fake_images_a = gen_a(images_b, training=True)\n",
    "            fake_images_b = gen_b(images_a, training=True)\n",
    "            recov_images_a = gen_a(fake_images_b, training=True)\n",
    "            recov_images_b = gen_b(fake_images_a, training=True)\n",
    "            # Discriminateur\n",
    "            disc_output_a = disc_a(fake_images_a, training=True)\n",
    "            disc_output_b = disc_b(fake_images_b, training=True)\n",
    "            # Identity\n",
    "            id_a = gen_a(images_a, training=True)\n",
    "            id_b = gen_b(images_b, training=True)\n",
    "\n",
    "            # 1. Cycle consistency loss\n",
    "            cc_loss = loss_bce(images_a, recov_images_a) + loss_bce(images_b, recov_images_b)\n",
    "            # 2. Discriminateur loss\n",
    "            real_labels_a = tf.ones_like(disc_output_a)\n",
    "            real_labels_b = tf.ones_like(disc_output_b)\n",
    "            disc_loss = loss_bce(real_labels_a, disc_output_a) + loss_bce(real_labels_b, disc_output_b) # On veux tromper le discriminateur\n",
    "            # 3. Identity loss\n",
    "            id_loss = loss_bce(images_a, id_a) + loss_bce(images_b, id_b)\n",
    "\n",
    "\n",
    "            total_g_loss = COEF_CYC * cc_loss + COEF_AV * disc_loss + COEF_ID * id_loss\n",
    "            loss_data.update({\n",
    "                \"cc_loss\": COEF_CYC * cc_loss,\n",
    "                \"disc_loss\": COEF_AV * disc_loss,\n",
    "                \"id_loss\": COEF_ID * id_loss,\n",
    "            })\n",
    "\n",
    "            # Backward\n",
    "            vars_g = gen_a.trainable_variables + gen_b.trainable_variables\n",
    "            grads_g = gen_tape.gradient(total_g_loss, vars_g)\n",
    "            GENERATOR_OPTIMIZER.apply_gradients([(g, v) for g, v in zip(grads_g, vars_g) if g is not None])\n",
    "    \n",
    "    # === Discriminateur ===\n",
    "    if 'discriminator' in turn :\n",
    "        fake_images_a = gen_a(images_b, training=True) # Calcul hors tape pour éviter de calculer le gradient\n",
    "        fake_images_b = gen_b(images_a, training=True) # Calcul hors tape pour éviter de calculer le gradient\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            # Forward dans les discriminateurs\n",
    "            real_output_a = disc_a(images_a, training=True)\n",
    "            fake_output_a = disc_a(fake_images_a, training=True)\n",
    "\n",
    "            real_output_b = disc_b(images_b, training=True)\n",
    "            fake_output_b = disc_b(fake_images_b, training=True)\n",
    "            \n",
    "            # Discriminateur loss\n",
    "            real_labels_a = tf.ones_like(real_output_a)\n",
    "            fake_labels_a = tf.zeros_like(fake_output_a)\n",
    "            real_labels_b = tf.ones_like(real_output_b)\n",
    "            fake_labels_b = tf.zeros_like(fake_output_b)\n",
    "            disc_loss_a_real = loss_bce(real_labels_a, real_output_a) \n",
    "            disc_loss_a_fake = loss_bce(fake_labels_a, fake_output_a)\n",
    "            disc_loss_b_real = loss_bce(real_labels_b, real_output_b)\n",
    "            disc_loss_b_fake = loss_bce(fake_labels_b, fake_output_b)\n",
    "            \n",
    "            total_d_loss =  disc_loss_a_real + disc_loss_a_fake + disc_loss_b_real + disc_loss_b_fake\n",
    "            loss_data.update({\n",
    "                \"disc_loss_a_real\": disc_loss_a_real,\n",
    "                \"disc_loss_a_fake\": disc_loss_a_fake,\n",
    "                \"disc_loss_b_real\": disc_loss_b_real,\n",
    "                \"disc_loss_b_fake\": disc_loss_b_fake,\n",
    "            })\n",
    "\n",
    "            # Backward\n",
    "            vars_d = disc_a.trainable_variables + disc_b.trainable_variables\n",
    "            grads_d = disc_tape.gradient(total_d_loss, vars_d)\n",
    "            DISCRIMINATOR_OPTIMIZER.apply_gradients([(g, v) for g, v in zip(grads_d, vars_d) if g is not None])\n",
    "    \n",
    "    # --- Debug ---\n",
    "    for v in disc_a.trainable_variables + disc_b.trainable_variables:\n",
    "        tf.debugging.assert_all_finite(v, \"NaN/Inf in D weights\")\n",
    "\n",
    "    return total_g_loss, total_d_loss, loss_data\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "wait = 0\n",
    "\n",
    "# --- Entraînement ---\n",
    "progress_bar = trange(TOTAL_ITERATIONS, desc=\"Training\", leave=True)\n",
    "history = defaultdict(list)\n",
    "turn = ['generator', 'discriminator']\n",
    "for index in progress_bar:\n",
    "    # --- Chargement des images ---\n",
    "    images = next(TrainGen)\n",
    "    # --- Entraînement ---\n",
    "    Generators = (GeneratorModel_A, GeneratorModel_B)\n",
    "    Discriminators = (DiscriminatorModel_A, DiscriminatorModel_B)\n",
    "    g_loss, d_loss, loss_data = train_step(images, Generators, Discriminators, turn)\n",
    "    # --- Affichage ---\n",
    "    current_lr_gen = GENERATOR_OPTIMIZER._decayed_lr(tf.float32).numpy()\n",
    "    current_lr_disc = DISCRIMINATOR_OPTIMIZER._decayed_lr(tf.float32).numpy()\n",
    "    postfix = {\"g_loss\": f\"{g_loss:.4e}\", \"d_loss\": f\"{d_loss:.4e}\", \"lr_gen\": f\"{current_lr_gen:.4e}\", \"lr_disc\": f\"{current_lr_disc:.4e}\"}\n",
    "    loss_data =  {key: f\"{value:.4e}\" for key, value in loss_data.items()}\n",
    "    postfix.update(loss_data)\n",
    "    progress_bar.set_postfix(postfix)\n",
    "    # --- Sauvegarde des pertes ---\n",
    "    for key, value in loss_data.items() :\n",
    "        history[key].append(float(value))\n",
    "    # --- Changement de tour ---\n",
    "    turn = TRAINING_CYCLE[index % len(TRAINING_CYCLE)]\n",
    "\n",
    "    # --- Test ---\n",
    "    if index % 1000 == 0:\n",
    "        images_a, images_b = images\n",
    "        fake_images_a = GeneratorModel_A(images_b, training=False)\n",
    "        fake_images_b = GeneratorModel_B(images_a, training=False)\n",
    "        recov_images_a = GeneratorModel_A(fake_images_b, training=False)\n",
    "        recov_images_b = GeneratorModel_B(fake_images_a, training=False)\n",
    "        fig, axes = plt.subplots(2,3, figsize=(18, 5))\n",
    "        axes[0][0].imshow(images_a[0])\n",
    "        axes[0][0].axis('off')\n",
    "        axes[0][0].set_title(\"A original\")\n",
    "        axes[0][1].imshow(fake_images_b[0])\n",
    "        axes[0][1].axis('off')\n",
    "        axes[0][1].set_title(\"fake B\")\n",
    "        axes[1][0].imshow(images_b[0])\n",
    "        axes[1][0].axis('off')\n",
    "        axes[1][0].set_title(\"B original\")\n",
    "        axes[1][1].imshow(fake_images_a[0])\n",
    "        axes[1][1].axis('off')\n",
    "        axes[1][1].set_title(\"fake A\")\n",
    "        axes[0][2].imshow(recov_images_a[0])\n",
    "        axes[0][2].axis('off')\n",
    "        axes[0][2].set_title(\"recov A\")\n",
    "        axes[1][2].imshow(recov_images_b[0])\n",
    "        axes[1][2].axis('off')\n",
    "        axes[1][2].set_title(\"recov B\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # --- Sauvegarde du modèle ---\n",
    "    if index % 10000 == 0 and index != 0:\n",
    "        save_model(GeneratorModel_A, f\"models/cyclegan/GeneratorAModel.h5\")\n",
    "        save_model(GeneratorModel_B, f\"models/cyclegan/GeneratorBModel.h5\")\n",
    "        save_model(DiscriminatorModel_A, f\"models/cyclegan/DiscriminatorAModel.h5\")\n",
    "        save_model(DiscriminatorModel_B, f\"models/cyclegan/DiscriminatorBModel.h5\")\n",
    "        # Save history\n",
    "        history_path = f\"models/cyclegan/history.json\"\n",
    "        with open(history_path, 'w') as f:\n",
    "            json.dump(history, f, indent=4)\n",
    "        print(f\"Models saved at iteration {index}\")\n",
    "        # Plot history\n",
    "        plot_history(history, span=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab7e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# History\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5324e3b3",
   "metadata": {},
   "source": [
    "### Inférence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbbf265",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = next(iter(TrainGen))\n",
    "images_a, images_b = images\n",
    "fake_images_a = GeneratorModel_A(images_b, training=False)\n",
    "fake_images_b = GeneratorModel_B(images_a, training=False)\n",
    "recov_images_a = GeneratorModel_A(fake_images_b, training=False)\n",
    "recov_images_b = GeneratorModel_B(fake_images_a, training=False)\n",
    "fig, axes = plt.subplots(2,3, figsize=(18, 5))\n",
    "axes[0][0].imshow(images_a[0])\n",
    "axes[0][0].axis('off')\n",
    "axes[0][0].set_title(\"A original\")\n",
    "axes[0][1].imshow(fake_images_b[0])\n",
    "axes[0][1].axis('off')\n",
    "axes[0][1].set_title(\"fake B\")\n",
    "axes[1][0].imshow(images_b[0])\n",
    "axes[1][0].axis('off')\n",
    "axes[1][0].set_title(\"B original\")\n",
    "axes[1][1].imshow(fake_images_a[0])\n",
    "axes[1][1].axis('off')\n",
    "axes[1][1].set_title(\"fake A\")\n",
    "axes[0][2].imshow(recov_images_a[0])\n",
    "axes[0][2].axis('off')\n",
    "axes[0][2].set_title(\"recov A\")\n",
    "axes[1][2].imshow(recov_images_b[0])\n",
    "axes[1][2].axis('off')\n",
    "axes[1][2].set_title(\"recov B\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
