{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion\n",
    "Dataset : https://www.kaggle.com/datasets/mohannadaymansalah/stable-diffusion-dataaaaaaaaa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import random as r\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPUs detected: {len(gpus)}\")\n",
    "        print(f\"GPUs: {gpus}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPUs detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (512, 512, 3)\n",
    "BATCH_SIZE = 16\n",
    "STEPS = 1000 # Nombre d'étapes de diffusion\n",
    "BETA_1 = 1e-4\n",
    "BETA_T = 2e-2\n",
    "PATH_DATASETS = ['stable-diffusion-face-dataset/512/man','stable-diffusion-face-dataset/512/woman']\n",
    "VALIDATION_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.1\n",
    "EPOCHS = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data préprosessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_images(paths, target_size):\n",
    "    data = []\n",
    "    for path in paths:\n",
    "        for img_name in os.listdir(path):\n",
    "            img_path = os.path.join(path, img_name)\n",
    "            img = load_img(img_path, target_size=target_size)\n",
    "            img_array = img_to_array(img, dtype='uint8')\n",
    "            data.append(img_array)\n",
    "    return np.array(data, dtype='uint8')\n",
    "\n",
    "data = load_and_preprocess_images(PATH_DATASETS, INPUT_SHAPE[:2])\n",
    "print(f\"Loaded {len(data)} images with shape {data.shape} and dtype {data.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = r.randint(0, len(data))\n",
    "plt.imshow(data[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data préprosessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = np.split(data, [int(len(data)*(1-VALIDATION_SPLIT-TEST_SPLIT)), int(len(data)*(1-TEST_SPLIT))])\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Validation data shape: {val_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian noice tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_images = 10\n",
    "\n",
    "class LinearNoiceScheduler:\n",
    "    def __init__(self):\n",
    "        self.betas = np.linspace(BETA_1, BETA_T, STEPS)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.sqrt_alphas = np.sqrt(self.alphas)\n",
    "        self.c_alphas = np.cumprod(self.alphas)\n",
    "        self.sqrt_c_alphas = np.sqrt(self.c_alphas)\n",
    "        self.sqrt_one_minus_c_alphas = np.sqrt(1 - self.c_alphas)\n",
    "    \n",
    "    def add_noise(self, original_images, noise, step):\n",
    "        original_shape = original_images.shape\n",
    "\n",
    "        sqrt_alpha_cumprod = self.sqrt_c_alphas[step]\n",
    "        sqrt_one_minus_alpha_cumprod = self.sqrt_one_minus_c_alphas[step]\n",
    "\n",
    "        noisy_image = sqrt_alpha_cumprod*original_images + sqrt_one_minus_alpha_cumprod*noise\n",
    "        return noisy_image\n",
    "\n",
    "    def sampler(self, noisy_image, noise_pred, step):\n",
    "        x0 = ( noisy_image - self.sqrt_one_minus_c_alphas[step]*noise_pred ) / self.sqrt_c_alphas[step]\n",
    "        mean = (noisy_image - (self.betas[step] * noise_pred)) / self.sqrt_one_minus_c_alphas[step]\n",
    "        mean = mean / self.sqrt_alphas[step]\n",
    "\n",
    "        if step == 0:\n",
    "            return mean, x0\n",
    "        \n",
    "        variance = (1 - self.c_alphas[step-1]) / (1 - self.c_alphas[step])\n",
    "        variance = variance * self.betas[step]\n",
    "        sigma = variance ** 0.5\n",
    "        z = np.random.normal(size=noisy_image.shape)\n",
    "        return mean + sigma * z, x0\n",
    "\n",
    "\n",
    "NoiceScheduler = LinearNoiceScheduler()\n",
    "\n",
    "# Sélectionner nb_images aléatoires du dataset\n",
    "selected_images = data[np.random.choice(len(data), nb_images, replace=False)] / 255.0\n",
    "noise = np.random.normal(size=selected_images.shape[1:])\n",
    "\n",
    "# Initialiser les listes pour stocker les moyennes et les écarts types\n",
    "means = []\n",
    "stds = []\n",
    "\n",
    "step_jump = 10\n",
    "\n",
    "# Calculer la moyenne et l'écart type à chaque étape\n",
    "for step in range(0, STEPS, step_jump):\n",
    "    noisy_images = NoiceScheduler.add_noise(selected_images, noise, step)\n",
    "    means.append(np.mean(noisy_images))\n",
    "    stds.append(np.std(noisy_images))\n",
    "\n",
    "# Plot des résultats\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(0, STEPS, step_jump), means, label='Mean')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Mean')\n",
    "plt.title('Mean of Noisy Images')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(0, STEPS, step_jump), stds, label='Standard Deviation')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Standard Deviation')\n",
    "plt.title('Standard Deviation of Noisy Images')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# On prend quelques images aléatoires\n",
    "idx = r.randint(1, len(selected_images)-1)\n",
    "test_images = selected_images[:idx]\n",
    "\n",
    "# Show noise\n",
    "plt.figure(figsize=(6, 6))\n",
    "noise_img = np.clip(noise, 0, 1)\n",
    "plt.imshow(noise_img)\n",
    "plt.title('Noise')\n",
    "plt.axis('off')\n",
    "# Show noise distribution\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.hist(noise.flatten(), bins=150)\n",
    "plt.title('Noise Distribution')\n",
    "plt.xlabel('Intensity')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of cumprod_alphas\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(NoiceScheduler.c_alphas)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Cumprod alpha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre d'images bruitées à afficher\n",
    "bruitage = 14\n",
    "\n",
    "test_image = test_images[0]\n",
    "jump = STEPS // bruitage\n",
    "# Show noisying and denoising of one of the test images\n",
    "fig, axes = plt.subplots(2, bruitage+1, figsize=(20, 4))\n",
    "for slot in range(bruitage+1):\n",
    "    #print('slot :', slot)\n",
    "    step = min((slot+1) * jump, STEPS-1)\n",
    "    #print('step :', step)\n",
    "    c_alpha = NoiceScheduler.c_alphas[step]\n",
    "    noisy_images = NoiceScheduler.add_noise(test_image, noise, step)\n",
    "    info, denoiced_image = NoiceScheduler.sampler(noisy_images, noise, step)\n",
    "    noisy_image_plt = np.clip(noisy_images, 0, 1)\n",
    "    denoiced_image_plt = np.clip(denoiced_image, 0, 1)\n",
    "    axes[0,slot].imshow(noisy_image_plt)\n",
    "    axes[1,slot].imshow(denoiced_image_plt)\n",
    "    axes[0,slot].axis('off')\n",
    "    axes[1,slot].axis('off')\n",
    "    axes[0,slot].set_title(f'{step} - {c_alpha:.2f}')\n",
    "    axes[1,slot].set_title(f'{step} - {c_alpha:.2f}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test of positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    # position : un tableau des positions\n",
    "    # d_model : la dimensionnalité des embeddings\n",
    "    angle_rates = 1 / np.power(10000, (2 * (np.arange(d_model) // 2)) / np.d_model)\n",
    "    angle_rads = position[:, np.newaxis] * angle_rates[np.newaxis, :]\n",
    "    \n",
    "    # Appliquer sin sur les indices pairs, cos sur les indices impairs\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # sin pour les indices pairs\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # cos pour les indices impairs\n",
    "\n",
    "    return angle_rads\n",
    "\n",
    "def conv_block(input_tensor, num_filters):\n",
    "    x = layers.Conv2D(num_filters, (3, 3), padding=\"same\")(input_tensor)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    \n",
    "    x = layers.Conv2D(num_filters, (3, 3), padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def encoder_block(input_tensor, num_filters):\n",
    "    x = conv_block(input_tensor, num_filters)\n",
    "    p = layers.MaxPooling2D((2, 2))(x)\n",
    "    return x, p\n",
    "\n",
    "def decoder_block(input_tensor, skip_features, num_filters):\n",
    "    x = layers.Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input_tensor)\n",
    "    x = layers.Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def build_unet(input_shape):\n",
    "    inputs = layers.Input(input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    s1, p1 = encoder_block(inputs, 64)\n",
    "    s2, p2 = encoder_block(p1, 128)\n",
    "    s3, p3 = encoder_block(p2, 256)\n",
    "    s4, p4 = encoder_block(p3, 512)\n",
    "\n",
    "    # Bottleneck\n",
    "    b = conv_block(p4, 1024)\n",
    "\n",
    "    # Decoder\n",
    "    d1 = decoder_block(b, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "\n",
    "    outputs = layers.Conv2D(3, (1, 1), activation=\"sigmoid\")(d4)  # output is a 3-channel image (RGB)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Construire le modèle\n",
    "unet_model = build_unet(INPUT_SHAPE)\n",
    "unet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(data, batch_size):\n",
    "    while True:\n",
    "        idx = r.sample(range(len(data)), batch_size)\n",
    "        yield data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
