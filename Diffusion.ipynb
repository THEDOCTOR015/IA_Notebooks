{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion DDPM\n",
    "Datasets : MSCOCO 2014, MNIST et CelebA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.utils import plot_model, Sequence\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import random as r\n",
    "from tqdm import tqdm # progress bar\n",
    "import time\n",
    "import datetime\n",
    "from IPython.display import clear_output\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPUs detected: {len(gpus)}\")\n",
    "        print(f\"GPUs: {gpus}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPUs detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "ANNOTDIR = 'datasets/annotations_trainval2014'\n",
    "DATADIR = 'datasets/train2014'\n",
    "INSTANCEFILE = '{}/annotations/instances_{}.json'.format(ANNOTDIR, DATADIR)\n",
    "\n",
    "# Choice of dataset\n",
    "CHOSEN_DATASET = input('mnist, coco or face ? (default: mnist) ') \n",
    "if CHOSEN_DATASET == 'coco':\n",
    "    # On utilise le dataset MS COCO 2014\n",
    "    INPUT_SHAPE = (224, 224, 3)\n",
    "    COCO_INSTANCES = COCO(INSTANCEFILE)\n",
    "    IDS = COCO_INSTANCES.getImgIds()\n",
    "    r.shuffle(IDS)\n",
    "    print(f'Number of images in the dataset: {len(IDS)}')\n",
    "    BATCH_SIZE = 16\n",
    "    OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "    PATIENCE = 2\n",
    "elif CHOSEN_DATASET == 'face':\n",
    "    # On utilise le dataset de visages\n",
    "    INPUT_SHAPE = (64, 64, 3)\n",
    "    BATCH_SIZE = 32*4\n",
    "    OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    IDS = [ f'datasets/Humans_Face/{element}' for element in os.listdir('datasets/Humans_Face') ]\n",
    "    IDS += [ f'datasets/celeba/{element}' for element in os.listdir('datasets/celeba') ]\n",
    "    r.shuffle(IDS)\n",
    "    PATIENCE = 2\n",
    "else:\n",
    "    # On utilise le dataset MNIST\n",
    "    INPUT_SHAPE = (32, 32, 1) # On passe de 28x28 à 32x32 pour avoir une taille multiple de 2\n",
    "    BATCH_SIZE = 32\n",
    "    OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    IDS = []\n",
    "    PATIENCE = 2\n",
    "\n",
    "STEPS = 1000 # Nombre d'étapes de diffusion\n",
    "BETA_1 = 1e-4\n",
    "BETA_T = 1e-2\n",
    "#PATH_DATASETS = ['stable-diffusion-face-dataset/512/man','stable-diffusion-face-dataset/512/woman']\n",
    "TRAIN_SPLIT = 0.8\n",
    "VALIDATION_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.05\n",
    "EPOCHS = 1000 # L'early stopping est utilisé\n",
    "\n",
    "# Configuration de l'augmentation\n",
    "DATA_GEN = ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "BASE_GEN = ImageDataGenerator()\n",
    "\n",
    "class LinearNoiceScheduler:\n",
    "    def __init__(self):\n",
    "        self.betas = np.linspace(BETA_1, BETA_T, STEPS) # on a donc b1, b2, ..., bT (T=STEPS), /!\\ self.betas est en 0-index alors que les étapes sont en 1-index\n",
    "        self.sqrt_betas = np.sqrt(self.betas)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.sqrt_alphas = np.sqrt(self.alphas)\n",
    "        self.c_alphas = np.cumprod(self.alphas)\n",
    "        self.sqrt_c_alphas = np.sqrt(self.c_alphas)\n",
    "        self.sqrt_one_minus_c_alphas = np.sqrt(1 - self.c_alphas)\n",
    "    \n",
    "    def add_noise(self, original_images, steps, return_noise=False):\n",
    "        c_step = steps - 1 # steps est en 1-indexed et c_step en 0-indexed\n",
    "        original_shape = original_images.shape\n",
    "        noise = np.random.normal(size=original_shape)\n",
    "\n",
    "        sqrt_alpha_cumprod = self.sqrt_c_alphas[c_step]\n",
    "        sqrt_one_minus_alpha_cumprod = self.sqrt_one_minus_c_alphas[c_step]\n",
    "        sqrt_alpha_cumprod = sqrt_alpha_cumprod[ :, np.newaxis, np.newaxis, np.newaxis]\n",
    "        sqrt_one_minus_alpha_cumprod = sqrt_one_minus_alpha_cumprod[ :, np.newaxis, np.newaxis, np.newaxis]\n",
    "\n",
    "        noisy_image = sqrt_alpha_cumprod*original_images + sqrt_one_minus_alpha_cumprod*noise\n",
    "\n",
    "        #noisy_image = np.clip(noisy_image, -1, 1) # TEST : on clip les valeurs pour être sûr qu'elles soient dans [-1, 1]\n",
    "        if return_noise:\n",
    "            return noisy_image, noise\n",
    "        return noisy_image\n",
    "\n",
    "    def remove_noise(self, noisy_image, noise_pred, step):\n",
    "        \"\"\"\n",
    "        step : l'étape actuel de la diffusion, (l'image bruité est donc à l'étape step)\n",
    "\n",
    "        updates_image : l'image à l'étape step-1\n",
    "        \"\"\"\n",
    "        c_step = step - 1 # steps est en 1-indexed et c_step en 0-indexed\n",
    "        # Récupération des paramètres pour l'étape t\n",
    "        alpha_t = self.alphas[c_step]\n",
    "        sqrt_alpha_t = np.sqrt(alpha_t)\n",
    "        sqrt_one_minus_cum_alpha_t = self.sqrt_one_minus_c_alphas[c_step]\n",
    "\n",
    "        alpha_factor = ( 1 - alpha_t ) / sqrt_one_minus_cum_alpha_t\n",
    "        \n",
    "        # Calcul de l'image mise à jour x_{t-1}\n",
    "        updated_image = (1 / sqrt_alpha_t) * (noisy_image - (alpha_factor * noise_pred))\n",
    "        \n",
    "        # Ajout du terme de stabilité pour les étapes non finales\n",
    "        if step > 1:\n",
    "            noise = np.random.normal(size=noisy_image.shape)\n",
    "        else:\n",
    "            noise = np.zeros(noisy_image.shape)\n",
    "        \n",
    "        sigma_t = self.sqrt_betas[c_step]\n",
    "        updated_image += sigma_t * noise\n",
    "        \n",
    "        return updated_image\n",
    "\n",
    "def inference(for_plot=False, plot_step=50, image=None, num_steps=STEPS):\n",
    "    res = []\n",
    "    batch_image = np.expand_dims(np.random.normal(size=INPUT_SHAPE), axis=0) if image is None else np.expand_dims(image, axis=0)\n",
    "    with tqdm(total=num_steps, desc='Inference', unit='step') as pbar:\n",
    "        for step in range(num_steps, 0, -1):\n",
    "            if step % plot_step == 0:\n",
    "                res.append((step, batch_image[0]))\n",
    "            numpy_step = np.array([step])\n",
    "            batch_step = np.array([[step]]) / STEPS # Normalisation de l'étape\n",
    "            noise_pred = model.predict((batch_image, batch_step), verbose=0)\n",
    "            batch_image = NoiceScheduler.remove_noise(batch_image, noise_pred, numpy_step)\n",
    "            pbar.update()\n",
    "    res.append((step, batch_image[0]))\n",
    "    if for_plot:\n",
    "        return res\n",
    "    return batch_image[0]\n",
    "\n",
    "def plot_inference_gif():\n",
    "    batch_image = np.expand_dims(np.random.normal(size=INPUT_SHAPE), axis=0)\n",
    "    for step in range(STEPS, 0, -1):\n",
    "        clear_output(wait=True)\n",
    "        batch_step = np.array([[step]]) / STEPS # Normalisation de l'étape\n",
    "        noise_pred = model.predict((batch_image, batch_step), verbose=0)\n",
    "        batch_image = NoiceScheduler.remove_noise(batch_image, noise_pred, np.array([step]))\n",
    "        plot_image = (batch_image[0] + 1) / 2\n",
    "        plt.imshow(np.clip(plot_image, 0, 1))\n",
    "        plt.title(f'{step}')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "def plot_inference(image=None):\n",
    "    result = inference(for_plot=True, image=image)\n",
    "    fig, axes = plt.subplots(1, len(result), figsize=(20, 5))\n",
    "    for i, res in enumerate(result):\n",
    "        step, img = res\n",
    "        img = (img + 1) / 2 # Convertion de -1, 1 à 0, 1\n",
    "        axes[i].imshow(np.clip(img, 0, 1))\n",
    "        axes[i].set_title(f'{int(step)}')\n",
    "        axes[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def array_stats(array, plot=False):\n",
    "    print(f'Shape: {array.shape} \\n \\\n",
    "        Mean: {np.mean(array)} \\n \\\n",
    "        Min: {np.min(array)} \\n \\\n",
    "        Max: {np.max(array)} \\n \\\n",
    "        Std: {np.std(array)}')\n",
    "    if plot:\n",
    "        sns.histplot(array.flatten())\n",
    "        plt.show()\n",
    "\n",
    "# Loss function\n",
    "@tf.function\n",
    "def loss_fn(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "NoiceScheduler = LinearNoiceScheduler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data préprosessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO Dataset\n",
    "class DatasetGeneratorCOCO(Sequence):\n",
    "    def _getsplit(self, ensemble):\n",
    "        if ensemble == 'train':\n",
    "            start = 0\n",
    "            stop = int(TRAIN_SPLIT * len(IDS))\n",
    "        elif ensemble == 'val':\n",
    "            start = int(TRAIN_SPLIT * len(IDS))\n",
    "            stop = int((TRAIN_SPLIT + VALIDATION_SPLIT) * len(IDS))\n",
    "        elif ensemble == 'test':\n",
    "            start = int((TRAIN_SPLIT + VALIDATION_SPLIT) * len(IDS))\n",
    "            stop = len(IDS)\n",
    "        return start, stop\n",
    "\n",
    "    def __init__(self, ensemble, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.ensemble = ensemble\n",
    "        \n",
    "        # Créer une liste de tous les IDs d'images\n",
    "        start, stop = self._getsplit(ensemble)\n",
    "        self.ids = IDS[start:stop]\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.ids) / BATCH_SIZE))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_ids = self.ids[index * BATCH_SIZE : (index + 1) * BATCH_SIZE]\n",
    "        batch_noisy_images = []\n",
    "        batch_steps = []\n",
    "        batch_noises = []\n",
    "        for id in batch_ids:\n",
    "            # Charger l'image\n",
    "            file_name = COCO_INSTANCES.imgs[id]['file_name']\n",
    "            image = Image.open(f'{DATADIR}/{file_name}')\n",
    "            image = image.resize((224, 224))\n",
    "            image = image.convert('RGB')\n",
    "            image = img_to_array(image)\n",
    "            image = (image / 255.0) * 2 - 1 # Normalisation entre -1 et 1\n",
    "            if self.ensemble == 'train':\n",
    "                image = DATA_GEN.random_transform(image)\n",
    "            else :\n",
    "                image = BASE_GEN.random_transform(image)\n",
    "            # Créer le label en rajoutant du bruit\n",
    "            step = np.array([np.random.randint(1, STEPS)])\n",
    "            noisy_image, noise_used = NoiceScheduler.add_noise(np.expand_dims(image, axis=0), step, return_noise=True)\n",
    "            batch_noisy_images.append(noisy_image[0])\n",
    "            batch_steps.append(step)\n",
    "            batch_noises.append(noise_used[0])\n",
    "\n",
    "        batch_noisy_images = np.array(batch_noisy_images, dtype='float64')\n",
    "        batch_steps = np.array(batch_steps, dtype='float64') / STEPS\n",
    "        batch_noises = np.array(batch_noises, dtype='float64')\n",
    "\n",
    "        return (batch_noisy_images, batch_steps, batch_noises)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.ensemble == 'train':\n",
    "            np.random.shuffle(self.ids)\n",
    "\n",
    "# Face Dataset\n",
    "class DatasetGeneratorFace(Sequence):\n",
    "    def _getsplit(self, ensemble):\n",
    "        if ensemble == 'train':\n",
    "            start = 0\n",
    "            stop = int(TRAIN_SPLIT * len(IDS))\n",
    "        elif ensemble == 'val':\n",
    "            start = int(TRAIN_SPLIT * len(IDS))\n",
    "            stop = int((TRAIN_SPLIT + VALIDATION_SPLIT) * len(IDS))\n",
    "        elif ensemble == 'test':\n",
    "            start = int((TRAIN_SPLIT + VALIDATION_SPLIT) * len(IDS))\n",
    "            stop = len(IDS)\n",
    "        return start, stop\n",
    "    \n",
    "    def __init__(self, ensemble, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.ensemble = ensemble\n",
    "        \n",
    "        # Créer une liste de tous les IDs d'images\n",
    "        start, stop = self._getsplit(ensemble)\n",
    "        self.ids = IDS[start:stop]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.ids) / BATCH_SIZE))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_ids = self.ids[index * BATCH_SIZE : (index + 1) * BATCH_SIZE]\n",
    "        batch_noisy_images = []\n",
    "        batch_steps = []\n",
    "        batch_noises = []\n",
    "        for id in batch_ids:\n",
    "            # Charger l'image\n",
    "            image = load_img(id, target_size=(INPUT_SHAPE[0], INPUT_SHAPE[1]), color_mode='rgb')\n",
    "            image = img_to_array(image)\n",
    "            image = (image / 255.0) * 2 - 1 # Normalisation entre -1 et 1\n",
    "            if self.ensemble == 'train':\n",
    "                image = DATA_GEN.random_transform(image)\n",
    "            else :\n",
    "                image = BASE_GEN.random_transform(image)\n",
    "            # Créer le label en rajoutant du bruit\n",
    "            step = np.array([np.random.randint(1, STEPS)])\n",
    "            noisy_image, noise_used = NoiceScheduler.add_noise(np.expand_dims(image, axis=0), step, return_noise=True)\n",
    "            batch_noisy_images.append(noisy_image[0])\n",
    "            batch_steps.append(step)\n",
    "            batch_noises.append(noise_used[0])\n",
    "        \n",
    "        batch_noisy_images = np.array(batch_noisy_images, dtype='float64')\n",
    "        batch_steps = np.array(batch_steps, dtype='float64') / STEPS\n",
    "        batch_noises = np.array(batch_noises, dtype='float64')\n",
    "\n",
    "        return (batch_noisy_images, batch_steps, batch_noises)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.ensemble == 'train': # Sert à rien de shuffle les données de validation et de test\n",
    "            np.random.shuffle(self.ids)\n",
    "    \n",
    "    def get_random_images(self, n):\n",
    "        images = []\n",
    "        for _ in range(n):\n",
    "            id = r.choice(self.ids)\n",
    "            image = load_img(f'Humans_Face/{id}', target_size=(INPUT_SHAPE[0], INPUT_SHAPE[1]))\n",
    "            image = img_to_array(image)\n",
    "            image = (image / 255.0) * 2 - 1 # Normalisation entre -1 et 1\n",
    "            images.append(image)\n",
    "        return images\n",
    "    \n",
    "# MNIST Dataset\n",
    "class DatasetGeneratorMNIST(Sequence):\n",
    "    def _getsplit(self, ensemble):\n",
    "        if ensemble == 'train':\n",
    "            start = 0\n",
    "            stop = int(TRAIN_SPLIT * len(self.ids))\n",
    "        elif ensemble == 'val':\n",
    "            start = int(TRAIN_SPLIT * len(self.ids))\n",
    "            stop = int((TRAIN_SPLIT + VALIDATION_SPLIT) * len(self.ids))\n",
    "        elif ensemble == 'test':\n",
    "            start = int((TRAIN_SPLIT + VALIDATION_SPLIT) * len(self.ids))\n",
    "            stop = len(self.ids)\n",
    "        return start, stop\n",
    "\n",
    "    def __init__(self, ensemble, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.ensemble = ensemble\n",
    "        \n",
    "        # Créer une liste de tous les IDs d'images\n",
    "        self.dict_training = { i : [] for i in range(10) }\n",
    "        for i in range(10):\n",
    "            # On récupère les images de chaque classe\n",
    "            self.dict_training[i] = [ f'MNIST/training/{i}/' + element for element in os.listdir(f'MNIST/training/{i}') ]\n",
    "        self.dict_validation = { i : [] for i in range(10) }\n",
    "        for i in range(10):\n",
    "            # On récupère les images de chaque classe\n",
    "            self.dict_validation[i] = [ f'MNIST/validation/{i}/' + element for element in os.listdir(f'MNIST/validation/{i}') ]\n",
    "        # On fusionne les deux dictionnaires\n",
    "        self.dict = { i : [] for i in range(10) }\n",
    "        for i in range(10):\n",
    "            self.dict[i] = self.dict_training[i] + self.dict_validation[i]\n",
    "        self.ids = []\n",
    "        if IDS == []:\n",
    "            for i in range(10):\n",
    "                self.ids += self.dict[i]\n",
    "            # Shuffle\n",
    "            r.shuffle(self.ids)\n",
    "        else:\n",
    "            self.ids = IDS\n",
    "        start, stop = self._getsplit(ensemble)\n",
    "        self.ids = self.ids[start:stop]\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.ids) / BATCH_SIZE))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_ids = self.ids[index * BATCH_SIZE : (index + 1) * BATCH_SIZE]\n",
    "        batch_noise = []\n",
    "        batch_noisy_image = []\n",
    "        batch_steps = []\n",
    "        for id in batch_ids:\n",
    "            # Charger l'image\n",
    "            image = load_img(id, color_mode='grayscale', target_size=(INPUT_SHAPE[0], INPUT_SHAPE[1]))\n",
    "            image = img_to_array(image)\n",
    "            image = ((image / 255.0) * 2) - 1 # Normalisation entre -1 et 1\n",
    "            # Créer le label en rajoutant du bruit\n",
    "            step = np.array([np.random.randint(1, STEPS)])\n",
    "            batch_steps.append(step)\n",
    "            noisy_image, noise_used = NoiceScheduler.add_noise(np.expand_dims(image, axis=0), step, return_noise=True)\n",
    "            batch_noise.append(noise_used[0])\n",
    "            batch_noisy_image.append(noisy_image[0])\n",
    "\n",
    "        batch_noise = np.array(batch_noise, dtype='float64')\n",
    "        batch_noisy_image = np.array(batch_noisy_image, dtype='float64')\n",
    "        batch_steps = np.array(batch_steps, dtype='float64') / STEPS # Normalisation entre 0 et 1\n",
    "\n",
    "        return (batch_noisy_image, batch_steps, batch_noise)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.ensemble == 'train':\n",
    "            np.random.shuffle(self.ids)\n",
    "    \n",
    "    def get_random_images(self, n):\n",
    "        images = []\n",
    "        for _ in range(n):\n",
    "            id = r.choice(self.ids)\n",
    "            image = load_img(id, color_mode='grayscale', target_size=(INPUT_SHAPE[0], INPUT_SHAPE[1]))\n",
    "            image = img_to_array(image)\n",
    "            image = ((image / 255.0) * 2) - 1 # Normalisation entre -1 et 1\n",
    "            images.append(image)\n",
    "        return images\n",
    "\n",
    "if CHOSEN_DATASET == 'coco':\n",
    "    train_generator = DatasetGeneratorCOCO('train')\n",
    "    val_generator = DatasetGeneratorCOCO('val')\n",
    "    test_generator = DatasetGeneratorCOCO('test')\n",
    "elif CHOSEN_DATASET == 'face':\n",
    "    train_generator = DatasetGeneratorFace('train')\n",
    "    val_generator = DatasetGeneratorFace('val')\n",
    "    test_generator = DatasetGeneratorFace('test')\n",
    "else:\n",
    "    train_generator = DatasetGeneratorMNIST('train')\n",
    "    val_generator = DatasetGeneratorMNIST('val')\n",
    "    test_generator = DatasetGeneratorMNIST('test')\n",
    "\n",
    "print(f'Taille du dataset d\\'entrainement: {len(train_generator)} batches, {len(train_generator.ids)} items')\n",
    "print(f'Taille du dataset de validation: {len(val_generator)} batches, {len(val_generator.ids)} items')\n",
    "print(f'Taille du dataset de test: {len(test_generator)} batches, {len(test_generator.ids)} items')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Sélectionner un batch aléatoire\n",
    "batch_index = np.random.randint(0, len(train_generator))\n",
    "noisy_images, steps, noises = train_generator.__getitem__(batch_index)\n",
    "print(f'Noisy_images shape: {noisy_images.shape}')\n",
    "print(f'Noises shape: {noises.shape}')\n",
    "\n",
    "# Sélectionner une image aléatoire dans le batch\n",
    "image_index = np.random.randint(0, len(noisy_images))\n",
    "noisy_image = noisy_images[image_index] # On prend le premier élément de l'axe des batchs\n",
    "noisy_image = (noisy_image + 1) / 2 # On remet les valeurs de pixels entre 0 et 1\n",
    "noise = noises[image_index] # On prend le premier élément de l'axe des batchs\n",
    "print(f'Noisy_image shape: {noisy_images.shape}')\n",
    "print(f'Noise shape: {noises.shape}')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].imshow(np.clip(noisy_image, 0, 1))\n",
    "axes[0].set_title('Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(np.clip(noise, 0, 1))\n",
    "axes[1].set_title(f'Noise')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian noice tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sélectionne un batch aléatoire\n",
    "r_batch = r.randint(0, len(train_generator))\n",
    "batch = train_generator[r_batch]\n",
    "selected_images, selected_steps, selected_labels = batch\n",
    "noise = np.random.normal(size=selected_images.shape[1:])\n",
    "\n",
    "# Initialiser les listes pour stocker les moyennes et les écarts types\n",
    "means = []\n",
    "stds = []\n",
    "\n",
    "step_jump = 10\n",
    "\n",
    "# Calculer la moyenne et l'écart type à chaque étape\n",
    "for step in range(0, STEPS, step_jump):\n",
    "    step = np.array([step])\n",
    "    noisy_images = NoiceScheduler.add_noise(selected_images, step)\n",
    "    means.append(np.mean(noisy_images))\n",
    "    stds.append(np.std(noisy_images))\n",
    "\n",
    "# Plot des résultats\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(0, STEPS, step_jump), means, label='Mean')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Mean')\n",
    "plt.title('Mean of Noisy Images')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(0, STEPS, step_jump), stds, label='Standard Deviation')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Standard Deviation')\n",
    "plt.title('Standard Deviation of Noisy Images')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# On prend quelques images aléatoires\n",
    "idx = r.randint(1, len(selected_images)-1)\n",
    "test_images = selected_images[:idx]\n",
    "\n",
    "# Show noise\n",
    "plt.figure(figsize=(6, 6))\n",
    "noise_img = np.clip(noise, 0, 1)\n",
    "plt.imshow(noise_img)\n",
    "plt.title('Noise')\n",
    "plt.axis('off')\n",
    "# Show noise distribution\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.hist(noise.flatten(), bins=150)\n",
    "plt.title('Noise Distribution')\n",
    "plt.xlabel('Intensity')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of cumprod_alphas\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(NoiceScheduler.c_alphas)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Cumprod alpha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sélectionne une image aléatoire\n",
    "r_image = r.randint(0, len(val_generator)-1)\n",
    "batch = val_generator[r_image]\n",
    "test_images, test_step,test_labels = batch # A PATCH\n",
    "test_image = test_images[0]\n",
    "jump = STEPS // 10\n",
    "fig, axes = plt.subplots(1, 11, figsize=(20, 4))\n",
    "for slot in range(11):\n",
    "    step = slot * jump\n",
    "    step = np.array([step])\n",
    "    if step == 0:\n",
    "        step = np.array([1])\n",
    "    c_alpha = NoiceScheduler.c_alphas[step-2] # step est 1-indexed\n",
    "    noisy_images = NoiceScheduler.add_noise(test_image, step)\n",
    "    noisy_images = (noisy_images + 1) / 2\n",
    "    noisy_image_plt = np.clip(noisy_images, 0, 1)\n",
    "    axes[slot].imshow(noisy_image_plt[0])\n",
    "    axes[slot].axis('off')\n",
    "    axes[slot].set_title(f'{int(step)} - {float(c_alpha):.2f}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions\n",
    "\n",
    "def conv_block(input_tensor, num_filters):\n",
    "    x = layers.Conv2D(num_filters, (3, 3), padding=\"same\")(input_tensor)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    \n",
    "    x = layers.Conv2D(num_filters, (3, 3), padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def resnet_block(input_tensor, num_filters, block_num):\n",
    "    input_tensor = layers.Conv2D(num_filters, (1, 1), padding=\"same\")(input_tensor)\n",
    "    for _ in range(block_num):\n",
    "        x = layers.Conv2D(num_filters, (3, 3), padding=\"same\")(input_tensor)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation(\"leaky_relu\")(x)\n",
    "    x = layers.Add()([x, input_tensor])  \n",
    "    return x\n",
    "\n",
    "def encoder_block(input_tensor, num_filters):\n",
    "    x = conv_block(input_tensor, num_filters)\n",
    "    p = layers.MaxPooling2D((2, 2))(x)\n",
    "    return x, p\n",
    "\n",
    "def decoder_block(input_tensor, skip_features, num_filters):\n",
    "    x = layers.Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input_tensor)\n",
    "    x = layers.Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def conv_block_time(input_tensor, num_filters, time_tensor):\n",
    "    # Convolution\n",
    "    x_params = layers.Conv2D(num_filters, (3, 3), padding=\"same\")(input_tensor)\n",
    "    x_params = layers.BatchNormalization()(x_params)\n",
    "    x_params = layers.Activation(\"leaky_relu\")(x_params)\n",
    "\n",
    "    # Time \n",
    "    x_time = layers.Dense(num_filters)(time_tensor)\n",
    "    x_time = layers.BatchNormalization()(x_time)\n",
    "    x_time = layers.Activation(\"leaky_relu\")(x_time)\n",
    "    x_time = layers.Reshape((1, 1, num_filters))(x_time)\n",
    "    x_params = x_params * x_time\n",
    "\n",
    "    # Convolution\n",
    "    x_out = layers.Conv2D(num_filters, (3, 3), padding=\"same\")(input_tensor)\n",
    "    x_out = layers.Add()([x_params, x_out])\n",
    "    x_out = layers.BatchNormalization()(x_out)\n",
    "    x_out = layers.Activation(\"leaky_relu\")(x_out)\n",
    "    \n",
    "    return x_out\n",
    "\n",
    "def self_attention_block(input_tensor, num_dims, num_heads=8, ff_dim=2):\n",
    "    #input_tensor = layers.Dense(num_dims)(input_tensor)\n",
    "    x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=num_dims, value_dim=num_dims)(input_tensor, input_tensor)\n",
    "    x = layers.Add()([x, input_tensor])\n",
    "    x2 = layers.LayerNormalization()(x)\n",
    "    x = layers.Dense(num_dims*ff_dim, activation='leaky_relu')(x2)\n",
    "    x = layers.Dense(num_dims, activation='leaky_relu')(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.Add()([x, x2])\n",
    "    return x\n",
    "\n",
    "def film_layer(input_tensor, step, num_filters):\n",
    "    gamma = layers.Dense(num_filters, activation='tanh')(step)\n",
    "    beta = layers.Dense(num_filters, activation='tanh')(step)\n",
    "\n",
    "    gamma = layers.Reshape((1, 1, num_filters))(gamma)  # New shape: [batch_size, 1, 1, num_filters]\n",
    "    beta = layers.Reshape((1, 1, num_filters))(beta)    # New shape: [batch_size, 1, 1, num_filters]\n",
    "    return input_tensor * gamma + beta\n",
    "\n",
    "def resnet_block_film(input_tensor, num_filters, block_num, step):\n",
    "    input_tensor = layers.Conv2D(num_filters, (1, 1), padding=\"same\")(input_tensor)\n",
    "    x = input_tensor\n",
    "    for _ in range(block_num):\n",
    "        x = layers.Conv2D(num_filters, (3, 3), padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = film_layer(x, step, num_filters) # Comme décrit dans le papier FiLM\n",
    "        x = layers.Activation(\"leaky_relu\")(x)\n",
    "    x = layers.Add()([x, input_tensor])\n",
    "    return x\n",
    "\n",
    "def attention_with_step(input_tensor, step, num_dims, num_heads=8, ff_dim=2):\n",
    "    step_emb = layers.Dense(num_dims)(step)\n",
    "    step_emb = layers.Reshape((int(num_dims**(1/2)), int(num_dims**(1/2),), 1))(step_emb)\n",
    "    query = layers.Add()([input_tensor, step_emb])\n",
    "    x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=num_dims, value_dim=num_dims)(query, input_tensor)\n",
    "    x = layers.Add()([x, input_tensor])\n",
    "    x2 = layers.LayerNormalization()(x)\n",
    "    x = layers.Dense(num_dims*ff_dim, activation='leaky_relu')(x2)\n",
    "    x = layers.Dense(num_dims, activation='leaky_relu')(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.Add()([x, x2])\n",
    "    return x\n",
    "\n",
    "\n",
    "'''\n",
    "MODELS\n",
    "'''\n",
    "\n",
    "# COCO\n",
    "\n",
    "def build_unetCOCO(input_shape):\n",
    "    inputs = layers.Input(input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    s1, p1 = encoder_block(inputs, 64)\n",
    "    s2, p2 = encoder_block(p1, 128)\n",
    "    s3, p3 = encoder_block(p2, 256)\n",
    "    s4, p4 = encoder_block(p3, 512)\n",
    "\n",
    "    # Bottleneck\n",
    "    b = conv_block(p4, 1024)\n",
    "\n",
    "    # Decoder\n",
    "    d1 = decoder_block(b, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "\n",
    "    d5 = layers.Concatenate()([d4, inputs])\n",
    "    outputs = layers.Conv2D(3, (1, 1))(d5)  # output is a 3-channel image (RGB)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name='Diffusion_UNetCOCO')\n",
    "    return model\n",
    "\n",
    "def build_unetCOCOv2(input_shape):\n",
    "\n",
    "    input_image = layers.Input(input_shape)\n",
    "\n",
    "    input_step = layers.Input((1,))\n",
    "    step = layers.Dense(128, activation='leaky_relu')(input_step)\n",
    "\n",
    "    # Encoder\n",
    "\n",
    "    s11 = conv_block_time(input_image, 64, step)\n",
    "    s12 = conv_block_time(s11, 64, step)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(s12)\n",
    "\n",
    "    s21 = conv_block_time(p1, 128, step)\n",
    "    s22 = conv_block_time(s21, 128, step)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(s22)\n",
    "\n",
    "    s31 = conv_block_time(p2, 256, step)\n",
    "    s32 = conv_block_time(s31, 256, step)\n",
    "    p3 = layers.MaxPooling2D((2, 2))(s32)\n",
    "\n",
    "    # Bottleneck\n",
    "\n",
    "    b= conv_block_time(p3, 512, step)\n",
    "    b = conv_block_time(b, 512, step)\n",
    "    b = layers.UpSampling2D((2, 2))(b)\n",
    "\n",
    "    # Decoder\n",
    "\n",
    "    d30 = layers.Concatenate()([b, s32])\n",
    "    d31 = conv_block_time(d30, 256, step)\n",
    "    d32 = conv_block_time(d31, 256, step)\n",
    "    d3 = layers.UpSampling2D((2, 2))(d32)\n",
    "\n",
    "    d20 = layers.Concatenate()([d3, s22])\n",
    "    d21 = conv_block_time(d20, 128, step)\n",
    "    d22 = conv_block_time(d21, 128, step)\n",
    "    d2 = layers.UpSampling2D((2, 2))(d22)\n",
    "\n",
    "    d10 = layers.Concatenate()([d2, s12])\n",
    "    d11 = conv_block_time(d10, 64, step)\n",
    "    d1 = conv_block_time(d11, 64, step)\n",
    "\n",
    "    d0 = layers.Concatenate()([d1, input_image])\n",
    "    outputs = layers.Conv2D(64, (3, 3), padding=\"same\")(d0)\n",
    "    outputs = layers.Conv2D(3, (1, 1))(outputs)  # output is a 3-channel image (RGB)\n",
    "\n",
    "    model = tf.keras.Model((input_image, input_step), outputs, name='Diffusion_UNetCOCOv2')\n",
    "    return model\n",
    "\n",
    "# FACE\n",
    "\n",
    "def build_unetFACEv2(input_shape):\n",
    "    input_image = layers.Input(input_shape) # 128x128x3\n",
    "    step = layers.Input((1,))\n",
    "\n",
    "    # Encoder\n",
    "    p1 = layers.AveragePooling2D((2, 2))(input_image) # 64x64x3\n",
    "    s11 = conv_block_time(p1, 128, step)\n",
    "    s12 = conv_block_time(s11, 128, step)\n",
    "\n",
    "    p2 = layers.AveragePooling2D((2, 2))(s12) # 32x32x64\n",
    "    s21 = conv_block_time(p2, 256, step)\n",
    "    s22 = conv_block_time(s21, 256, step)\n",
    "\n",
    "    p3 = layers.AveragePooling2D((2, 2))(s22) # 16x16x128\n",
    "    s31 = conv_block_time(p3, 512, step)\n",
    "    s32 = conv_block_time(s31, 512, step)\n",
    "\n",
    "    # Bottleneck\n",
    "    p4 = layers.AveragePooling2D((2, 2))(s32) # 8x8x256\n",
    "    \n",
    "    b = conv_block_time(p4, 1024, step)\n",
    "    b = conv_block_time(b, 1024, step)\n",
    "\n",
    "    # Decoder\n",
    "    d3 = layers.UpSampling2D((2, 2))(b)\n",
    "    d3 = layers.Concatenate()([d3, s32])\n",
    "    d31 = conv_block_time(d3, 512, step)\n",
    "    d32 = conv_block_time(d31, 512, step)\n",
    "\n",
    "    d2 = layers.UpSampling2D((2, 2))(d32)\n",
    "    d2 = layers.Concatenate()([d2, s22])\n",
    "    d21 = conv_block_time(d2, 256, step)\n",
    "    d22 = conv_block_time(d21, 256, step)\n",
    "\n",
    "    d1 = layers.UpSampling2D((2, 2))(d22)\n",
    "    d1 = layers.Concatenate()([d1, s12])\n",
    "    d11 = conv_block_time(d1, 128, step)\n",
    "    d12 = conv_block_time(d11, 128, step)\n",
    "\n",
    "    d0 = layers.UpSampling2D((2, 2))(d12)\n",
    "    d0 = layers.Concatenate()([d0, input_image])\n",
    "    outputs = layers.Conv2D(128, (3, 3), padding=\"same\")(d0)\n",
    "    outputs = layers.Conv2D(3, (1, 1))(outputs)  # output is a 3-channel image (RGB)\n",
    "\n",
    "    model = tf.keras.Model((input_image, step), outputs, name='Diffusion_UNetFACEv2')\n",
    "    return model\n",
    "\n",
    "def build_unetFACEv3(input_shape):\n",
    "    input_image = layers.Input(input_shape) # 128x128x3\n",
    "    step = layers.Input((1,))\n",
    "\n",
    "    # Encoder\n",
    "    p1 = layers.MaxPooling2D((2, 2))(input_image) # 64x64x3\n",
    "    s11 = conv_block_time(p1, 128, step)\n",
    "    s12 = conv_block_time(s11, 128, step)\n",
    "\n",
    "    p2 = layers.MaxPooling2D((2, 2))(s12) # 32x32x64\n",
    "    s21 = conv_block_time(p2, 256, step)\n",
    "    s22 = conv_block_time(s21, 256, step)\n",
    "\n",
    "    p3 = layers.MaxPooling2D((2, 2))(s22) # 16x16x128\n",
    "    s31 = conv_block_time(p3, 512, step)\n",
    "    s32 = conv_block_time(s31, 512, step)\n",
    "\n",
    "    # Bottleneck MLP\n",
    "    p4 = layers.MaxPooling2D((2, 2))(s32) # 8x8x256\n",
    "    \n",
    "    x = layers.Flatten()(p4)\n",
    "    x = layers.Concatenate()([x, step])\n",
    "    x = layers.Dense(2048, activation='leaky_relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Dense(8*8*128, activation='leaky_relu')(x)\n",
    "    x = layers.Reshape((8, 8, 128))(x)\n",
    "    b = layers.BatchNormalization()(x) # Bottleneck\n",
    "    \n",
    "    # Decoder\n",
    "    d3 = layers.UpSampling2D((2, 2))(b)\n",
    "    d3 = layers.Concatenate()([d3, s32])\n",
    "    d31 = conv_block_time(d3, 512, step)\n",
    "    d32 = conv_block_time(d31, 512, step)\n",
    "\n",
    "    d2 = layers.UpSampling2D((2, 2))(d32)\n",
    "    d2 = layers.Concatenate()([d2, s22])\n",
    "    d21 = conv_block_time(d2, 256, step)\n",
    "    d22 = conv_block_time(d21, 256, step)\n",
    "\n",
    "    d1 = layers.UpSampling2D((2, 2))(d22)\n",
    "    d1 = layers.Concatenate()([d1, s12])\n",
    "    d11 = conv_block_time(d1, 128, step)\n",
    "    d12 = conv_block_time(d11, 128, step)\n",
    "\n",
    "    d0 = layers.UpSampling2D((2, 2))(d12)\n",
    "    d0 = layers.Concatenate()([d0, input_image])\n",
    "    outputs = layers.Conv2D(128, (3, 3), padding=\"same\")(d0)\n",
    "    outputs = layers.Conv2D(3, (1, 1))(outputs)  # output is a 3-channel image (RGB)\n",
    "\n",
    "    model = tf.keras.Model((input_image, step), outputs, name='Diffusion_UNetFACEv2')\n",
    "    return model\n",
    "\n",
    "def build_unetFACEv4(input_shape):\n",
    "    '''\n",
    "    Best val loss : 0.023802\n",
    "    '''\n",
    "    input_image = layers.Input(input_shape) # 128x128x3\n",
    "    input_image = layers.Dropout(0.10)(input_image)\n",
    "    step = layers.Input((1,))\n",
    "\n",
    "    # Encoder\n",
    "    p1 = layers.MaxPooling2D((2, 2))(input_image) # 64x64x3\n",
    "    c1 = p1\n",
    "    for _ in range(4):\n",
    "        c1 = resnet_block_film(c1, 64, 2, step)\n",
    "\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c1) # 32x32x64\n",
    "    c2 = p2\n",
    "    for _ in range(4):\n",
    "        c2 = resnet_block_film(p2, 128, 2, step)\n",
    "\n",
    "\n",
    "    p3 = layers.MaxPooling2D((2, 2))(c2) # 16x16x128\n",
    "    c3 = p3\n",
    "    for _ in range(4):\n",
    "        c3 = resnet_block_film(c3, 256, 2, step)\n",
    "        #c3 = attention_with_step(c3, step, 16*16, 8, 2)\n",
    "\n",
    "    p4 = layers.MaxPooling2D((2, 2))(c3) # 8x8x256\n",
    "    c4 = p4\n",
    "    for _ in range(4):\n",
    "        c4 = resnet_block_film(c4, 512, 2, step)\n",
    "    \n",
    "\n",
    "    # Bottleneck\n",
    "    b_num_filters = 256\n",
    "    b_step = layers.Dense(512, activation='leaky_relu')(step)\n",
    "    b = layers.Conv2D(b_num_filters, (1, 1))(c4)\n",
    "    b = layers.Flatten()(b)\n",
    "    b = layers.Concatenate()([b, b_step])\n",
    "    b = layers.Dropout(0.45)(b)\n",
    "\n",
    "    b = layers.Dense(2024, activation='leaky_relu')(b) # True bottleneck\n",
    "    b = layers.BatchNormalization()(b)\n",
    "    b = layers.Dropout(0.45)(b)\n",
    "\n",
    "    b = layers.Dense(2024*4, activation='leaky_relu')(b)\n",
    "    b = layers.BatchNormalization()(b)\n",
    "    b = layers.Dropout(0.45)(b)\n",
    "    \n",
    "    b = layers.Dense(2024, activation='leaky_relu')(b) # True bottleneck\n",
    "    b = layers.BatchNormalization()(b)\n",
    "    b = layers.Dropout(0.45)(b)\n",
    "\n",
    "    b = layers.Dense(8*8*b_num_filters, activation='leaky_relu')(b)\n",
    "    b = layers.Reshape((8, 8, b_num_filters))(b)\n",
    "    b = layers.BatchNormalization()(b)\n",
    "    b = layers.Dropout(0.45)(b)\n",
    "\n",
    "    # Decoder\n",
    "    d4 = b\n",
    "    d4 = layers.Concatenate()([d4, c4])\n",
    "    for _ in range(4):\n",
    "        d4 = resnet_block_film(d4, 512, 2, step)\n",
    "    \n",
    "    d3 = layers.UpSampling2D((2, 2))(d4)\n",
    "    d3 = layers.Concatenate()([d3, c3])\n",
    "    for _ in range(4):\n",
    "        d3 = resnet_block_film(d3, 256, 2, step)\n",
    "        #d3 = attention_with_step(d3, step, 16*16, 8, 2)\n",
    "\n",
    "    d2 = layers.UpSampling2D((2, 2))(d3)\n",
    "    d2 = layers.Concatenate()([d2, c2])\n",
    "    for _ in range(4):\n",
    "        d2 = resnet_block_film(d2, 128, 2, step)\n",
    "\n",
    "    d1 = layers.UpSampling2D((2, 2))(d2)\n",
    "    d1 = layers.Concatenate()([d1, c1])\n",
    "    for _ in range(4):\n",
    "        d1 = resnet_block_film(d1, 64, 2, step)\n",
    "\n",
    "    d0 = layers.UpSampling2D((2, 2))(d1)\n",
    "    outputs = layers.Concatenate()([d0, input_image])\n",
    "    for _ in range(4):\n",
    "        outputs = resnet_block_film(outputs, 64, 2, step)\n",
    "    outputs = layers.Conv2D(3, (1, 1))(outputs)  # output is a 3-channel image (RGB)\n",
    "\n",
    "    model = tf.keras.Model((input_image, step), outputs, name='Diffusion_UNetFACEv4')\n",
    "    description = 'CNN U-Net with FiLM layers and ResNet blocks and a bottleneck MLP'\n",
    "    ModelManager.save_model(model, description)\n",
    "    return model\n",
    "\n",
    "def build_unetFACEv5(input_shape):\n",
    "    \"\"\"\n",
    "    Best val loss : 0.025702\n",
    "    \"\"\"\n",
    "    input_image = layers.Input(input_shape) # 128x128x3 \n",
    "    step = layers.Input((1,))\n",
    "\n",
    "    # Encoder\n",
    "    p1 = layers.MaxPooling2D((2, 2))(input_image) # 64x64x3\n",
    "    c1 = p1\n",
    "    for _ in range(4):\n",
    "        c1 = resnet_block_film(c1, 64, 2, step)\n",
    "\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c1) # 32x32x64\n",
    "    c2 = p2\n",
    "    for _ in range(4):\n",
    "        c2 = resnet_block_film(p2, 128, 2, step)\n",
    "\n",
    "\n",
    "    positions_256 = tf.expand_dims(tf.range(0, 256, 1), axis=0)\n",
    "    positionnal_encoding_256 = layers.Embedding(\n",
    "    input_dim=256,  # Maximum index + 1\n",
    "    output_dim=16*16,  # Each position index will be embedded into a n-dimensional vector\n",
    ")(positions_256)\n",
    "    positionnal_encoding_256 = layers.Reshape((16, 16, 256))(positionnal_encoding_256)\n",
    "\n",
    "    p3 = layers.MaxPooling2D((2, 2))(c2) # 16x16x128\n",
    "    c3 = p3\n",
    "    for _ in range(5):\n",
    "        c3 = resnet_block_film(c3, 256, 2, step)\n",
    "        c3 = layers.Add()([c3, positionnal_encoding_256])\n",
    "        c3 = layers.Permute((3, 1, 2))(c3) # Permute to match the shape of the attention mechanism (nums_dims, h, w)\n",
    "        c3 = layers.Reshape((c3.shape[1], c3.shape[2] * c3.shape[3]))(c3) # Reshape to match the shape of the attention mechanism (num_dims, h*w)\n",
    "        c3 = self_attention_block(c3, 16*16, 8, 2)\n",
    "        c3 = layers.Permute((2, 1))(c3)\n",
    "        c3 = layers.Reshape((16, 16, 256))(c3) # Reshape to match the shape of the input tensor\n",
    "\n",
    "    positions_512 = tf.expand_dims(tf.range(0, 512, 1), axis=0)\n",
    "    positionnal_encoding_512 = layers.Embedding(\n",
    "    input_dim=512,  # Maximum index + 1\n",
    "    output_dim=8*8,  # Each position index will be embedded into a 1-dimensional vector\n",
    ")(positions_512)\n",
    "    positionnal_encoding_512 = layers.Reshape((8, 8, 512))(positionnal_encoding_512)\n",
    "\n",
    "    p4 = layers.MaxPooling2D((2, 2))(c3) # 8x8x256\n",
    "    c4 = p4\n",
    "    for _ in range(6):\n",
    "        c4 = resnet_block_film(c4, 512, 2, step)\n",
    "        c4 = layers.Add()([c4, positionnal_encoding_512])\n",
    "        c4 = layers.Permute((3, 1, 2))(c4) \n",
    "        c4 = layers.Reshape((c4.shape[1], c4.shape[2] * c4.shape[3]))(c4)\n",
    "        c4 = self_attention_block(c4, 8*8, 8, 2)\n",
    "        c4 = layers.Permute((2, 1))(c4)\n",
    "        c4 = layers.Reshape((8, 8, 512))(c4)\n",
    "    \n",
    "\n",
    "    # Bottleneck\n",
    "    b_num_filters = 256\n",
    "    b_step = layers.Dense(512, activation='leaky_relu')(step)\n",
    "    b = layers.Conv2D(b_num_filters, (1, 1))(c4)\n",
    "    b = layers.Flatten()(b)\n",
    "    b = layers.Concatenate()([b, b_step])\n",
    "    b = layers.Dropout(0.35)(b)\n",
    "\n",
    "    b = layers.Dense(1024, activation='leaky_relu')(b) # True bottleneck\n",
    "    b = layers.BatchNormalization()(b)\n",
    "    b = layers.Dropout(0.35)(b)\n",
    "\n",
    "    b = layers.Dense(1024*4, activation='leaky_relu')(b)\n",
    "    b = layers.BatchNormalization()(b)\n",
    "    b = layers.Dropout(0.35)(b)\n",
    "    \n",
    "    b = layers.Dense(1024, activation='leaky_relu')(b) # True bottleneck\n",
    "    b = layers.BatchNormalization()(b)\n",
    "    b = layers.Dropout(0.35)(b)\n",
    "\n",
    "    b = layers.Dense(8*8*b_num_filters, activation='leaky_relu')(b)\n",
    "    b = layers.Reshape((8, 8, b_num_filters))(b)\n",
    "    b = layers.BatchNormalization()(b)\n",
    "    b = layers.Dropout(0.35)(b)\n",
    "\n",
    "    # Decoder\n",
    "    d4 = b\n",
    "    d4 = layers.Concatenate()([d4, c4])\n",
    "    for _ in range(6):\n",
    "        d4 = resnet_block_film(d4, 512, 2, step)\n",
    "        d4 = layers.Add()([d4, positionnal_encoding_512])\n",
    "        d4 = layers.Permute((3, 1, 2))(d4)\n",
    "        d4 = layers.Reshape((d4.shape[1], d4.shape[2] * d4.shape[3]))(d4)\n",
    "        d4 = self_attention_block(d4, 8*8, 8, 2)\n",
    "        d4 = layers.Permute((2, 1))(d4)\n",
    "        d4 = layers.Reshape((8, 8, 512))(d4)\n",
    "    \n",
    "    d3 = layers.UpSampling2D((2, 2))(d4)\n",
    "    d3 = layers.Concatenate()([d3, c3])\n",
    "    for _ in range(5):\n",
    "        d3 = resnet_block_film(d3, 256, 2, step)\n",
    "        d3 = layers.Add()([d3, positionnal_encoding_256])\n",
    "        d3 = layers.Permute((3, 1, 2))(d3)\n",
    "        d3 = layers.Reshape((d3.shape[1], d3.shape[2] * d3.shape[3]))(d3)\n",
    "        d3 = self_attention_block(d3, 16*16, 8, 2)\n",
    "        d3 = layers.Permute((2, 1))(d3)\n",
    "        d3 = layers.Reshape((16, 16, 256))(d3)\n",
    "\n",
    "    d2 = layers.UpSampling2D((2, 2))(d3)\n",
    "    d2 = layers.Concatenate()([d2, c2])\n",
    "    for _ in range(4):\n",
    "        d2 = resnet_block_film(d2, 128, 2, step)\n",
    "\n",
    "    d1 = layers.UpSampling2D((2, 2))(d2)\n",
    "    d1 = layers.Concatenate()([d1, c1])\n",
    "    for _ in range(4):\n",
    "        d1 = resnet_block_film(d1, 64, 2, step)\n",
    "\n",
    "    d0 = layers.UpSampling2D((2, 2))(d1)\n",
    "    outputs = layers.Concatenate()([d0, input_image])\n",
    "    for _ in range(4):\n",
    "        outputs = resnet_block_film(outputs, 64, 2, step)\n",
    "    outputs = layers.Conv2D(3, (1, 1))(outputs)  # output is a 3-channel image (RGB)\n",
    "\n",
    "    model = tf.keras.Model((input_image, step), outputs, name='Diffusion_UNetFACEv5')\n",
    "    description = 'CNN U-Net with FiLM layers and ResNet blocks and a attention mechanism bottleneck MLP'\n",
    "    ModelManager.save_model(model, description)\n",
    "    return model\n",
    "\n",
    "def build_unetFACEv6(input_shape):\n",
    "    input_image = layers.Input(input_shape)\n",
    "    input_image = layers.Dropout(0.00)(input_image)\n",
    "    step = layers.Input((1,))\n",
    "    step_expanded = tf.reduce_sum(step)\n",
    "\n",
    "    # Encoder\n",
    "    p0 = input_image\n",
    "    for _ in range(4):\n",
    "        p0 = resnet_block(p0, 64, 3)\n",
    "    p1 = layers.Conv2D(128, (3, 3), padding=\"same\", strides=2)(p0)\n",
    "    p1 = layers.BatchNormalization()(p1)\n",
    "    p1 = layers.Activation(\"leaky_relu\")(p1)\n",
    "\n",
    "    for _ in range(4):\n",
    "        p1 = resnet_block(p1, 128, 3)\n",
    "    p2 = layers.Conv2D(256, (3, 3), padding=\"same\", strides=2)(p1)\n",
    "    p2 = layers.BatchNormalization()(p2)\n",
    "    p2 = layers.Activation(\"leaky_relu\")(p2)\n",
    "\n",
    "    for _ in range(4):\n",
    "        p2 = resnet_block(p2, 256, 3)\n",
    "    p3 = layers.Conv2D(512, (3, 3), padding=\"same\", strides=2)(p2)\n",
    "    p3 = layers.BatchNormalization()(p3)\n",
    "    p3 = layers.Activation(\"leaky_relu\")(p3)\n",
    "\n",
    "    for _ in range(4):\n",
    "        p3 = resnet_block(p3, 512, 3)\n",
    "    \n",
    "    # Bottleneck\n",
    "    b = layers.Conv2D(1024, (3, 3), padding=\"same\", strides=2)(p3)\n",
    "    b = layers.BatchNormalization()(b)\n",
    "    b = layers.Activation(\"leaky_relu\")(b)\n",
    "    for _ in range(2):\n",
    "        b = resnet_block(b, 1024, 3)\n",
    "    b = layers.Conv2D(1024, (3, 3), padding=\"same\")(b)\n",
    "    b = layers.BatchNormalization()(b)\n",
    "    b = layers.Activation(\"leaky_relu\")(b)\n",
    "\n",
    "    # Decoder\n",
    "\n",
    "    d4 = layers.Conv2DTranspose(512, (3, 3), strides=2, padding=\"same\")(b)\n",
    "    d4 = layers.Concatenate()([d4, p3])\n",
    "    for _ in range(4):\n",
    "        d4 = resnet_block(d4, 512, 3)\n",
    "\n",
    "    d3 = layers.Conv2DTranspose(256, (3, 3), strides=2, padding=\"same\")(d4)\n",
    "    d3 = layers.Concatenate()([d3, p2])\n",
    "    for _ in range(4):\n",
    "        d3 = resnet_block(d3, 256, 3)\n",
    "\n",
    "    d2 = layers.Conv2DTranspose(128, (3, 3), strides=2, padding=\"same\")(d3)\n",
    "    d2 = layers.Concatenate()([d2, p1])\n",
    "    for _ in range(4):\n",
    "        d2 = resnet_block(d2, 128, 3)\n",
    "    \n",
    "    d1 = layers.Conv2DTranspose(64, (3, 3), strides=2, padding=\"same\")(d2)\n",
    "    d1 = layers.Concatenate()([d1, p0])\n",
    "    for _ in range(4):\n",
    "        d1 = resnet_block(d1, 64, 3)\n",
    "    \n",
    "\n",
    "    outputs = layers.Conv2D(3, (1, 1))(d1)  # output is a 3-channel image (RGB)\n",
    "    outputs = outputs + 0 * step_expanded\n",
    "\n",
    "    model = tf.keras.Model((input_image, step), outputs, name='Diffusion_UNetFACEv6')\n",
    "    return model\n",
    "\n",
    "# MNIST\n",
    "\n",
    "def build_unetMNIST(input_shape):\n",
    "    inputs = layers.Input(input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    s1, p1 = encoder_block(inputs, 16)\n",
    "    s2, p2 = encoder_block(p1, 32)\n",
    "    s3, p3 = encoder_block(p2, 64)\n",
    "\n",
    "    # Bottleneck\n",
    "    b = conv_block(p3, 128)\n",
    "\n",
    "    # Decoder\n",
    "    d1 = decoder_block(b, s3, 64)\n",
    "    d2 = decoder_block(d1, s2, 32)\n",
    "    d3 = decoder_block(d2, s1, 16)\n",
    "\n",
    "    d4 = layers.Concatenate()([d3, inputs])\n",
    "    outputs = layers.Conv2D(1, (1, 1))(d4)  # output is a 1-channel image (grayscale)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name='Diffusion_UNetMNIST')\n",
    "    return model\n",
    "\n",
    "def build_unetMNISTv2(input_shape):\n",
    "    input_image = layers.Input(input_shape)\n",
    "\n",
    "    # Encoder step\n",
    "    input_step = layers.Input((1,))\n",
    "    step = layers.Dense(512, activation='leaky_relu')(input_step)\n",
    "\n",
    "    # Encoder image\n",
    "    s11 = conv_block_time(input_image, 64, step)\n",
    "    s12 = conv_block_time(s11, 64, step)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(s12)\n",
    "\n",
    "    s21 = conv_block_time(p1, 128, step)\n",
    "    s22 = conv_block_time(s21, 128, step)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(s22)\n",
    "\n",
    "    s31 = conv_block_time(p2, 256, step)\n",
    "    s32 = conv_block_time(s31, 256, step)\n",
    "    p3 = layers.MaxPooling2D((2, 2))(s32)\n",
    "\n",
    "    s41 = conv_block_time(p3, 512, step)\n",
    "    s42 = conv_block_time(s41, 512, step)\n",
    "    p4 = layers.MaxPooling2D((2, 2))(s42)\n",
    "\n",
    "    # Bottleneck MLP\n",
    "    x = layers.Flatten()(p4)\n",
    "    x = layers.Concatenate()([x, step])\n",
    "    x = layers.Dense(512, activation='leaky_relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Dense(2*2*512, activation='leaky_relu')(x)\n",
    "    x = layers.Reshape((2, 2, 512))(x)\n",
    "    b = layers.BatchNormalization()(x) # Bottleneck\n",
    "\n",
    "    # Decoder\n",
    "    d41 = conv_block_time(b, 512, step)\n",
    "    d42 = conv_block_time(d41, 512, step)\n",
    "    d4 = layers.UpSampling2D((2, 2))(d42)\n",
    "    d4 = layers.Concatenate()([d4, s42])\n",
    "\n",
    "    d31 = conv_block_time(d4, 256, step)\n",
    "    d32 = conv_block_time(d31, 256, step)\n",
    "    d3 = layers.UpSampling2D((2, 2))(d32)\n",
    "    d3 = layers.Concatenate()([d3, s32])\n",
    "\n",
    "    d21 = conv_block_time(d3, 128, step)\n",
    "    d22 = conv_block_time(d21, 128, step)\n",
    "    d2 = layers.UpSampling2D((2, 2))(d22)\n",
    "    d2 = layers.Concatenate()([d2, s22])\n",
    "\n",
    "    d11 = conv_block_time(d2, 64, step)\n",
    "    d12 = conv_block_time(d11, 64, step)\n",
    "    d1 = layers.UpSampling2D((2, 2))(d12)\n",
    "    d1 = layers.Concatenate()([d1, s12])\n",
    "\n",
    "    d0 = layers.Concatenate()([d1, input_image])\n",
    "    outputs = layers.Conv2D(64, (3, 3), padding=\"same\")(d0)\n",
    "    outputs = layers.Conv2D(1, (1, 1))(outputs)  # output is a 1-channel image (grayscale)\n",
    "\n",
    "    model = tf.keras.Model((input_image,input_step), outputs, name='Diffusion_UNetMNISTv2')\n",
    "    return model\n",
    "\n",
    "def build_unetMNISTv2_nostep(input_shape):\n",
    "    input_image = layers.Input(input_shape)\n",
    "\n",
    "    # Encoder step\n",
    "    input_step = layers.Input((1,))\n",
    "\n",
    "    # Encoder image\n",
    "    s11 = conv_block(input_image, 64)\n",
    "    s12 = conv_block(s11, 64)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(s12)\n",
    "\n",
    "    s21 = conv_block(p1, 128)\n",
    "    s22 = conv_block(s21, 128)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(s22)\n",
    "\n",
    "    s31 = conv_block(p2, 256)\n",
    "    s32 = conv_block(s31, 256)\n",
    "    p3 = layers.MaxPooling2D((2, 2))(s32)\n",
    "\n",
    "    s41 = conv_block(p3, 512)\n",
    "    s42 = conv_block(s41, 512)\n",
    "    p4 = layers.MaxPooling2D((2, 2))(s42)\n",
    "\n",
    "    # Bottleneck MLP\n",
    "    x = layers.Flatten()(p4)\n",
    "    x = layers.Dense(512, activation='leaky_relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Dense(2*2*512, activation='leaky_relu')(x)\n",
    "    x = layers.Reshape((2, 2, 512))(x)\n",
    "    b = layers.BatchNormalization()(x) # Bottleneck\n",
    "\n",
    "    # Decoder\n",
    "    d41 = conv_block(b, 512)\n",
    "    d42 = conv_block(d41, 512)\n",
    "    d4 = layers.UpSampling2D((2, 2))(d42)\n",
    "    d4 = layers.Concatenate()([d4, s42])\n",
    "\n",
    "    d31 = conv_block(d4, 256)\n",
    "    d32 = conv_block(d31, 256)\n",
    "    d3 = layers.UpSampling2D((2, 2))(d32)\n",
    "    d3 = layers.Concatenate()([d3, s32])\n",
    "\n",
    "    d21 = conv_block(d3, 128)\n",
    "    d22 = conv_block(d21, 128)\n",
    "    d2 = layers.UpSampling2D((2, 2))(d22)\n",
    "    d2 = layers.Concatenate()([d2, s22])\n",
    "\n",
    "    d11 = conv_block(d2, 64)\n",
    "    d12 = conv_block(d11, 64)\n",
    "    d1 = layers.UpSampling2D((2, 2))(d12)\n",
    "\n",
    "    d0 = layers.Concatenate()([d1, input_image])\n",
    "    outputs = layers.Conv2D(32, (3, 3), padding=\"same\")(d0)\n",
    "    outputs = layers.Conv2D(1, (1, 1))(outputs)  # output is a 1-channel image (grayscale)\n",
    "\n",
    "    model = tf.keras.Model((input_image,input_step), outputs, name='build_unetMNISTv2_nostep')\n",
    "    return model\n",
    "\n",
    "def build_unetMNISTv3(input_shape):\n",
    "    input_image = layers.Input(input_shape)\n",
    "\n",
    "    # Encoder step\n",
    "    input_step = layers.Input((1,))\n",
    "    step = layers.Dense(256, activation='leaky_relu')(input_step)\n",
    "\n",
    "    # Encoder\n",
    "    s11 = conv_block(input_image, 64)\n",
    "    s12 = conv_block(s11, 64)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(s12)\n",
    "\n",
    "    s21 = conv_block(p1, 128)\n",
    "    s22 = conv_block(s21, 128)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(s22)\n",
    "\n",
    "    # Self-attention Bottleneck\n",
    "    s31 = self_attention_block(p2, 256)\n",
    "    s32 = self_attention_block(s31, 256)\n",
    "\n",
    "    # Decoder\n",
    "    d11 = conv_block(s32, 128)\n",
    "    d12 = conv_block(d11, 128)\n",
    "    d1 = layers.Concatenate()([d12, p2])\n",
    "    d1 = layers.UpSampling2D((2, 2))(d1)\n",
    "\n",
    "    d21 = conv_block(d1, 64)\n",
    "    d22 = conv_block(d21, 64)\n",
    "    d2 = layers.Concatenate()([d22, p1])\n",
    "    d2 = layers.UpSampling2D((2, 2))(d2)\n",
    "\n",
    "    d3 = layers.Concatenate()([d2, input_image])\n",
    "    output = layers.Conv2D(64, (3, 3), padding=\"same\")(d3)\n",
    "    output = layers.Conv2D(1, (1, 1))(output)  # output is a 1-channel image (grayscale)\n",
    "\n",
    "    model = tf.keras.Model((input_image, input_step), output, name='Diffusion_UNetMNISTv3')\n",
    "    return model\n",
    "\n",
    "\n",
    "# Construire le modèle\n",
    "if CHOSEN_DATASET == 'coco':\n",
    "    model = build_unetCOCOv2(INPUT_SHAPE)\n",
    "elif CHOSEN_DATASET == 'face':\n",
    "    model = build_unetFACEv6(INPUT_SHAPE)\n",
    "else:\n",
    "    model = build_unetMNISTv2(INPUT_SHAPE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True, show_layer_names=True, to_file=model.name+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "@tf.function\n",
    "def train_step(model, noisy_images, steps, noises, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model((noisy_images, steps), training=True)\n",
    "        predictions = tf.cast(predictions, tf.float64)\n",
    "        loss = loss_fn(noises, predictions) # y_true = noises, y_pred = predictions\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "epochs_losses = []\n",
    "val_epochs_losses = []\n",
    "min_loss = np.inf\n",
    "counter = 0\n",
    "model_save_name = f\"{model.name}_{datetime.datetime.now().strftime('%Y%m%d-%H%M')}.keras\"\n",
    "for epoch in range(EPOCHS):\n",
    "    # Entrainement du modèle\n",
    "    with tqdm(total=len(train_generator), desc=f'Epoch {epoch+1}/{EPOCHS}', unit='batch') as pbar:\n",
    "        epoch_losses = []\n",
    "        for step in range(len(train_generator)) :\n",
    "            # Récupérer les images et les bruits\n",
    "            noisy_images, steps, noises = train_generator[step]\n",
    "            # Training step\n",
    "            loss = train_step(model, noisy_images, steps, noises, OPTIMIZER)\n",
    "            # Métriques\n",
    "            epoch_losses.append(loss.numpy())\n",
    "            epoch_loss = np.mean(epoch_losses)\n",
    "            pbar.set_postfix(Loss=f\"{epoch_loss:.6f}\")\n",
    "            pbar.update()\n",
    "    train_generator.on_epoch_end()\n",
    "    epochs_losses.append(np.mean(epoch_losses))\n",
    "    # Validation\n",
    "    with tqdm(total=len(val_generator), desc=f'Validation {epoch+1}/{EPOCHS}', unit='batch') as pbar:\n",
    "        val_losses = []\n",
    "        for step in range(len(val_generator)):\n",
    "            noisy_images, steps, noises = val_generator[step]\n",
    "            predictions = model.predict((noisy_images, steps), verbose=0)\n",
    "            predictions = tf.cast(predictions, tf.float64)\n",
    "            loss = loss_fn(noises, predictions)\n",
    "            # Métriques\n",
    "            val_losses.append(loss.numpy())\n",
    "            pbar.set_postfix(Loss=f\"{np.mean(val_losses):.6f}\")\n",
    "            pbar.update()\n",
    "    val_epochs_losses.append(np.mean(val_losses))\n",
    "    # Early stopping\n",
    "    if val_epochs_losses[-1] >= min_loss and epoch > 0:\n",
    "        counter += 1\n",
    "        if counter >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    else:\n",
    "        counter = 0\n",
    "        min_loss = val_epochs_losses[-1]\n",
    "    # Affichage d'un test d'inférence\n",
    "    plot_inference()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour calculer les pertes pour chaque batch d'un générateur\n",
    "def calculate_losses(generator):\n",
    "    losses = []\n",
    "    with tqdm(total=len(generator), desc=f'Calculating Losses {generator.ensemble}', unit='batch') as pbar:\n",
    "        for step in range(len(generator)):\n",
    "            noisy_images, step, noises = generator[step]\n",
    "            pred_noises = model.predict((noisy_images, step), verbose=0)\n",
    "            pred_noises = tf.cast(pred_noises, tf.float64)\n",
    "            loss = loss_fn(noises, pred_noises)\n",
    "            losses.append(loss.numpy())\n",
    "            pbar.update()\n",
    "    return losses\n",
    "\n",
    "# Calcul des pertes pour chaque générateur\n",
    "train_losses = calculate_losses(train_generator)\n",
    "val_losses = calculate_losses(val_generator)\n",
    "test_losses = calculate_losses(test_generator)\n",
    "\n",
    "# Tracer les pertes en densité\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.kdeplot(train_losses, label='Train Losses', fill=True)\n",
    "sns.kdeplot(val_losses, label='Validation Losses', fill=True)\n",
    "sns.kdeplot(test_losses, label='Test Losses', fill=True)\n",
    "plt.xlabel('Loss')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density Plot of Losses')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculer la moyenne et l'écart type des pertes\n",
    "train_mean, train_std = np.mean(train_losses), np.std(train_losses)\n",
    "val_mean, val_std = np.mean(val_losses), np.std(val_losses)\n",
    "test_mean, test_std = np.mean(test_losses), np.std(test_losses)\n",
    "\n",
    "print(f'Train Loss - Mean: {train_mean:.6f}, Std: {train_std:.6f}')\n",
    "print(f'Validation Loss - Mean: {val_mean:.6f}, Std: {val_std:.6f}')\n",
    "print(f'Test Loss - Mean: {test_mean:.6f}, Std: {test_std:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('models\\diffusion\\diffusion_face_152M_24-11-24\\diffusion_face_152M_24-11-24.keras', compile=False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test itératif sur un bruit gaussien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test itératif sur un bruit gaussien (version fluide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_inference_gif()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Interpolation on 2 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "lambda_ = 0.5 # ratio de mélange entre les deux images\n",
    "chosen_step = 0.55 # étape de diffusion choisie (entre 0 => 0 et 1 => T)\n",
    "\n",
    "# Récupération des images\n",
    "images = test_generator.get_random_images(2)\n",
    "image1, image2 = images\n",
    "image1 = np.expand_dims(image1, axis=0)\n",
    "image2 = np.expand_dims(image2, axis=0)\n",
    "chosen_step = np.array([int(chosen_step * STEPS)])\n",
    "# Mixage des deux images\n",
    "mixed_image = lambda_*image1 + (1-lambda_)*image2\n",
    "# Ajout de bruit à l'image mixée\n",
    "noisy_mixed_image = NoiceScheduler.add_noise(mixed_image, chosen_step)\n",
    "# Diffusion de l'image mixée\n",
    "diffused_image = inference(image=noisy_mixed_image[0], num_steps=int(chosen_step))   \n",
    "# Plotting the original images, noisy images, and the diffused image\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "\n",
    "# Original images\n",
    "axes[0].imshow((image1[0] + 1) / 2, cmap='gray')\n",
    "axes[0].set_title('Original Image 1')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow((image2[0] + 1) / 2, cmap='gray')\n",
    "axes[1].set_title('Original Image 2')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Mixed image\n",
    "axes[2].imshow((mixed_image[0] + 1) / 2, cmap='gray')\n",
    "axes[2].set_title('Mixed Image')\n",
    "axes[2].axis('off')\n",
    "\n",
    "# Mixed noisy image\n",
    "axes[3].imshow(np.clip((noisy_mixed_image[0] + 1) / 2, 0, 1), cmap='gray')\n",
    "axes[3].set_title('Mixed Noisy Image')\n",
    "axes[3].axis('off')\n",
    "\n",
    "# Plotting the diffused image\n",
    "axes[4].imshow(np.clip((diffused_image + 1) / 2, 0, 1), cmap='gray')\n",
    "axes[4].set_title('Diffused Image')\n",
    "axes[4].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Interpolation on >2 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 5\n",
    "images = test_generator.get_random_images(num_images)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
