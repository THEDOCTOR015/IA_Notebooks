{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###\n",
    "### Initializers\n",
    "###\n",
    "\n",
    "class BaseInitializer:\n",
    "    def initialize(self, shape):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class RandomInitializer(BaseInitializer):\n",
    "    def initialize(self, shape):\n",
    "        return np.random.randn(*shape)\n",
    "\n",
    "class ZeroInitializer(BaseInitializer):\n",
    "    def initialize(self, shape):\n",
    "        return np.zeros(shape)\n",
    "\n",
    "###\n",
    "### Loss Functions\n",
    "###\n",
    "\n",
    "class BaseLoss:\n",
    "    def loss(self, y_pred, y_true):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def gradient(self, y_pred, y_true):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class MSELoss(BaseLoss):\n",
    "    def loss(self, y_pred, y_true):\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "    \n",
    "    def gradient(self, y_pred, y_true):\n",
    "        return 2 * (y_pred - y_true) / y_true.size\n",
    "\n",
    "###\n",
    "### Optimizers\n",
    "###\n",
    "\n",
    "class BaseOptimizer:\n",
    "    def update(self, layer, grad_weights, grad_biases):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SGD(BaseOptimizer):\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update(self, layer, grad_weights, grad_biases):\n",
    "        print(grad_weights, grad_biases, layer.weights, layer.biases, self.learning_rate)\n",
    "        if grad_weights is not None:\n",
    "            layer.weights -= self.learning_rate * grad_weights\n",
    "        if layer.use_bias and grad_biases is not None:\n",
    "            layer.biases -= self.learning_rate * grad_biases\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "### Layers\n",
    "###\n",
    "\n",
    "class BaseLayer:\n",
    "    def __init__(self):\n",
    "        self.input_shape = None\n",
    "        self.output_shape = None\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = input_shape\n",
    "        self.value = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_params_count(self):\n",
    "        return 0\n",
    "\n",
    "class InputLayer(BaseLayer):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "        if isinstance(input_shape, int):\n",
    "            input_shape = (input_shape,)\n",
    "        self.output_shape = input_shape\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return X\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {'input_shape': self.output_shape}\n",
    "\n",
    "class DenseLayer(BaseLayer):\n",
    "    def __init__(self, units, activation=None, use_bias=True, initializer=RandomInitializer()):\n",
    "        super().__init__()\n",
    "        if not isinstance(units, int):\n",
    "            raise ValueError(\"units must be an integer\")\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "        self.initializer = initializer\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        if isinstance(input_shape, int):\n",
    "            input_shape = (input_shape,)\n",
    "        self.input_shape = input_shape\n",
    "        self.weights = self.initializer.initialize((input_shape[-1], self.units))\n",
    "        self.biases = self.initializer.initialize((self.units,)) if self.use_bias else None\n",
    "        self.output_shape = (self.units,)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        if self.weights is None:\n",
    "            raise ValueError(\"La couche doit Ãªtre construite avant d'effectuer un forward pass.\")\n",
    "        \n",
    "        output = np.tensordot(X, self.weights, axes=[-1, 0])\n",
    "        if self.use_bias:\n",
    "            output += self.biases\n",
    "        \n",
    "        if self.activation:\n",
    "            output = self.activation.forward(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad_activation = grad_output\n",
    "        \n",
    "        grad_weights = np.tensordot(self.value.T, grad_activation, axes=1)\n",
    "        grad_biases = np.sum(grad_activation, axis=0) if self.use_bias else None\n",
    "        \n",
    "        grad_input = np.tensordot(grad_activation, self.weights.T, axes=1)\n",
    "        \n",
    "        return grad_input, grad_weights, grad_biases\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'units': self.units,\n",
    "            'input_shape': self.input_shape,\n",
    "            'weights_shape': self.weights.shape if self.weights is not None else None,\n",
    "            'biases_shape': self.biases.shape if self.biases is not None else None,\n",
    "            'use_bias': self.use_bias\n",
    "        }\n",
    "\n",
    "    def get_params_count(self):\n",
    "        param_count = self.weights.size if self.weights is not None else 0\n",
    "        if self.use_bias:\n",
    "            param_count += self.biases.size if self.biases is not None else 0\n",
    "        return param_count\n",
    "\n",
    "###\n",
    "### Activation Layers\n",
    "###\n",
    "\n",
    "class ActivationLayer(BaseLayer):\n",
    "    def __init__(self, activation):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = input_shape\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.activation.forward(X)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return self.activation.backward(self.value) * grad_output\n",
    "\n",
    "class ReLU(ActivationLayer):\n",
    "    def __init__(self):\n",
    "        super().__init__(activation=self)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def backward(self, X):\n",
    "        return (X > 0).astype(float) # gradient of ReLU\n",
    "\n",
    "class Sigmoid(ActivationLayer):\n",
    "    def __init__(self):\n",
    "        super().__init__(activation=self)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    \n",
    "    def backward(self, X):\n",
    "        return X * (1 - X) # gradient of sigmoid\n",
    "\n",
    "\n",
    "###\n",
    "### Model\n",
    "###\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.input_shape = None\n",
    "    \n",
    "    def add(self, layer):\n",
    "        if not self.layers:\n",
    "            self.input_shape = layer.output_shape\n",
    "        else:\n",
    "            layer.build(self.layers[-1].output_shape)\n",
    "        \n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def compile(self, optimizer, loss_function):\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function = loss_function\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "            layer.value = X\n",
    "        return X\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, DenseLayer):\n",
    "                loss_grad, grad_weights, grad_biases = layer.backward(loss_grad)\n",
    "                self.optimizer.update(layer, grad_weights, grad_biases)\n",
    "            else:\n",
    "                loss_grad = layer.backward(loss_grad)\n",
    "\n",
    "    def train(self, X, y, epochs, batch_size):\n",
    "        num_samples = X.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.random.permutation(num_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            for start in range(0, num_samples, batch_size):\n",
    "                end = start + batch_size\n",
    "                X_batch = X_shuffled[start:end]\n",
    "                y_batch = y_shuffled[start:end]\n",
    "                \n",
    "                output = self.forward(X_batch)\n",
    "                loss = self.loss_function.loss(output, y_batch)\n",
    "                loss_grad = self.loss_function.gradient(output, y_batch)\n",
    "                self.backward(loss_grad)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss}\")\n",
    "    \n",
    "    def summary(self):\n",
    "        print(\"\\nSummary:\")\n",
    "        print(\"=\" * 60)\n",
    "        total_params = 0\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            params = layer.get_params_count() if hasattr(layer, 'get_params_count') else 0\n",
    "            total_params += params\n",
    "            print(f\"Layer {i}: {layer.__class__.__name__}, Output Shape: {layer.output_shape}, Params: {params}\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Total Parameters: {total_params}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Model()\n",
    "model.add(InputLayer((10)))\n",
    "#model.add(DenseLayer(5))\n",
    "model.add(ReLU())\n",
    "model.add(DenseLayer(3))\n",
    "model.add(Sigmoid())\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.random.normal(size=model.input_shape)\n",
    "output = model.forward(input_data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.random.normal(size=(100, 10))\n",
    "y_train = np.ones((100, 3))\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=0.01), loss_function=MSELoss())\n",
    "model.train(X_train, y_train, epochs=100, batch_size=32)\n",
    "print(grad_weights, grad_biases, layer.weights, layer.biases, self.learning_rate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
